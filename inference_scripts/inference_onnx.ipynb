{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c07e1b-5fb8-43b4-9b67-d332a491ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8319cec-6ae0-471b-b6e4-aa3ee56327bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a2ef7b-b7d3-4cac-a05a-55e78f8ce00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set loaded from ./exported_test_set.csv\n",
      "X_test shape: (41992, 83)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Load the Exported Test Set\n",
    "# ---------------------------\n",
    "\n",
    "# Set the path to the exported test set CSV\n",
    "TEST_CSV = './exported_test_set.csv'\n",
    "\n",
    "if not os.path.exists(TEST_CSV):\n",
    "    raise FileNotFoundError(f\"Test set CSV not found at {TEST_CSV}\")\n",
    "\n",
    "# Load the test set from CSV\n",
    "X_test_export = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Assume the target column is named \"target\"\n",
    "Y_test = X_test_export[\"target\"].values\n",
    "X_test = X_test_export.drop(columns=[\"target\"])\n",
    "\n",
    "print(f\"Test set loaded from {TEST_CSV}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a2939f-ef5e-4c78-ae23-0771e8d0b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_transformed shape: (41992, 94)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Define the Preprocessor\n",
    "# ---------------------------\n",
    "# Identify columns (you must know these from training)\n",
    "numerical_cols = X_test.select_dtypes(include=[\"int64\", \"float64\", \"float32\"]).columns.tolist()\n",
    "categorical_cols = [\"proto\", \"service\"]\n",
    "\n",
    "# Create new transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on test set (or a subset) and transform test data\n",
    "X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "print(f\"X_test_transformed shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09fca89f-2ca6-40a7-b918-708e0789ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded successfully.\n",
      "ONNX model input name: input\n",
      "Expected input shape: [None, 94]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 3. Load ONNX Model and Inference \n",
    "# ---------------------------\n",
    "# Path to the saved ONNX model\n",
    "# onnx_model_path = \"./onnx_models/best_svm_rbf_only_model_100.onnx\"\n",
    "# onnx_model_path = \"./onnx_models/best_rf_model_100.onnx\"\n",
    "onnx_model_path = \"./onnx_models/best_lgb_model_100.onnx\"\n",
    "# onnx_model_path = \"./onnx_models/best_xgb_model_100.onnx\"\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    raise FileNotFoundError(f\"ONNX model not found at {onnx_model_path}\")\n",
    "\n",
    "# Create an ONNX Runtime inference session\n",
    "session = rt.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "input_name = session.get_inputs()[0].name\n",
    "print(\"ONNX model loaded successfully.\")\n",
    "print(\"ONNX model input name:\", input_name)\n",
    "print(\"Expected input shape:\", session.get_inputs()[0].shape)\n",
    "\n",
    "# Ensure the test set is in float32 (if not already)\n",
    "X_test_processed = X_test_transformed.astype(np.float32)\n",
    "\n",
    "# Choose a sample index (modify as needed)\n",
    "sample_index = 5  # for example, first sample\n",
    "sample = X_test_processed[sample_index:sample_index+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a666924c-7259-4bfe-9a70-103ad6bd5a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Single Sample Inference ---\n",
      "Sample index: 5\n",
      "True label: 3\n",
      "Predicted label: 3\n",
      "Predicted probabilities: [0.02855954 0.19437757 0.00094656 0.6490637  0.00503347 0.00750094\n",
      " 0.0050237  0.00407685 0.00269497 0.01220121 0.00369237 0.04601484\n",
      " 0.0408143 ]\n",
      "Accuracy (single sample): 1\n",
      "F1 Score (single sample): 1.0000\n",
      "Precision (single sample): 1.0000\n",
      "Recall (single sample): 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 5. Single Sample Inference and Metrics\n",
    "# ---------------------------\n",
    "\n",
    "# Choose a sample index (modify as needed)\n",
    "sample_index = 5  # for example, first sample\n",
    "\n",
    "onnx_outputs = session.run(None, {input_name: sample})\n",
    "# Assume the first output is the predicted label and the second (if exists) is the probabilities.\n",
    "predicted_label = onnx_outputs[0]\n",
    "predicted_probs = onnx_outputs[1] if len(onnx_outputs) > 1 else None\n",
    "\n",
    "# Print inference results for the sample\n",
    "true_label = Y_test[sample_index]\n",
    "print(\"\\n--- Single Sample Inference ---\")\n",
    "print(f\"Sample index: {sample_index}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label[0]}\")\n",
    "if predicted_probs is not None:\n",
    "    print(\"Predicted probabilities:\", predicted_probs[0])\n",
    "\n",
    "# For a single sample, accuracy is 1 if prediction matches, else 0.\n",
    "single_accuracy = 1 if predicted_label[0] == true_label else 0\n",
    "print(f\"Accuracy (single sample): {single_accuracy}\")\n",
    "\n",
    "# Compute additional metrics for the single sample (will be 0 or 1)\n",
    "y_true_single = np.array([true_label])\n",
    "y_pred_single = np.array([predicted_label[0]])\n",
    "\n",
    "f1 = f1_score(y_true_single, y_pred_single, average='macro')\n",
    "precision = precision_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"F1 Score (single sample): {f1:.4f}\")\n",
    "print(f\"Precision (single sample): {precision:.4f}\")\n",
    "print(f\"Recall (single sample): {recall:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Clean Up\n",
    "# ---------------------------\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf23a0f4-4fd6-42a3-a83b-e725c70194bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 100: 0.0233 ms\n",
      "Run 200: 0.0229 ms\n",
      "Run 300: 0.0235 ms\n",
      "Run 400: 0.0230 ms\n",
      "Run 500: 0.0229 ms\n",
      "Run 600: 0.0231 ms\n",
      "Run 700: 0.0227 ms\n",
      "Run 800: 0.0227 ms\n",
      "Run 900: 0.0229 ms\n",
      "Run 1000: 0.0233 ms\n",
      "Run 1100: 0.0228 ms\n",
      "Run 1200: 0.0227 ms\n",
      "Run 1300: 0.0229 ms\n",
      "Run 1400: 0.0229 ms\n",
      "Run 1500: 0.0226 ms\n",
      "Run 1600: 0.0226 ms\n",
      "Run 1700: 0.0232 ms\n",
      "Run 1800: 0.0228 ms\n",
      "Run 1900: 0.0229 ms\n",
      "Run 2000: 0.0230 ms\n",
      "Run 2100: 0.0229 ms\n",
      "Run 2200: 0.0227 ms\n",
      "Run 2300: 0.0231 ms\n",
      "Run 2400: 0.0229 ms\n",
      "Run 2500: 0.0229 ms\n",
      "Run 2600: 0.0231 ms\n",
      "Run 2700: 0.0227 ms\n",
      "Run 2800: 0.0223 ms\n",
      "Run 2900: 0.0230 ms\n",
      "Run 3000: 0.0226 ms\n",
      "Run 3100: 0.0228 ms\n",
      "Run 3200: 0.0227 ms\n",
      "Run 3300: 0.0224 ms\n",
      "Run 3400: 0.0226 ms\n",
      "Run 3500: 0.0229 ms\n",
      "Run 3600: 0.0227 ms\n",
      "Run 3700: 0.0226 ms\n",
      "Run 3800: 0.0229 ms\n",
      "Run 3900: 0.0227 ms\n",
      "Run 4000: 0.0228 ms\n",
      "Run 4100: 0.0228 ms\n",
      "Run 4200: 0.0226 ms\n",
      "Run 4300: 0.0227 ms\n",
      "Run 4400: 0.0226 ms\n",
      "Run 4500: 0.0228 ms\n",
      "Run 4600: 0.0227 ms\n",
      "Run 4700: 0.0227 ms\n",
      "Run 4800: 0.0228 ms\n",
      "Run 4900: 0.0227 ms\n",
      "Run 5000: 0.0226 ms\n",
      "Run 5100: 0.0226 ms\n",
      "Run 5200: 0.0227 ms\n",
      "Run 5300: 0.0224 ms\n",
      "Run 5400: 0.0225 ms\n",
      "Run 5500: 0.0226 ms\n",
      "Run 5600: 0.0230 ms\n",
      "Run 5700: 0.0226 ms\n",
      "Run 5800: 0.0228 ms\n",
      "Run 5900: 0.0231 ms\n",
      "Run 6000: 0.0224 ms\n",
      "Run 6100: 0.0226 ms\n",
      "Run 6200: 0.0229 ms\n",
      "Run 6300: 0.0227 ms\n",
      "Run 6400: 0.0226 ms\n",
      "Run 6500: 0.0228 ms\n",
      "Run 6600: 0.0230 ms\n",
      "Run 6700: 0.0229 ms\n",
      "Run 6800: 0.0228 ms\n",
      "Run 6900: 0.0229 ms\n",
      "Run 7000: 0.0226 ms\n",
      "Run 7100: 0.0227 ms\n",
      "Run 7200: 0.0226 ms\n",
      "Run 7300: 0.0225 ms\n",
      "Run 7400: 0.0226 ms\n",
      "Run 7500: 0.0227 ms\n",
      "Run 7600: 0.0230 ms\n",
      "Run 7700: 0.0227 ms\n",
      "Run 7800: 0.0225 ms\n",
      "Run 7900: 0.0229 ms\n",
      "Run 8000: 0.0226 ms\n",
      "Run 8100: 0.0228 ms\n",
      "Run 8200: 0.0229 ms\n",
      "Run 8300: 0.0231 ms\n",
      "Run 8400: 0.0228 ms\n",
      "Run 8500: 0.0230 ms\n",
      "Run 8600: 0.0234 ms\n",
      "Run 8700: 0.0227 ms\n",
      "Run 8800: 0.0231 ms\n",
      "Run 8900: 0.0227 ms\n",
      "Run 9000: 0.0234 ms\n",
      "Run 9100: 0.0228 ms\n",
      "Run 9200: 0.0229 ms\n",
      "Run 9300: 0.0232 ms\n",
      "Run 9400: 0.0229 ms\n",
      "Run 9500: 0.0228 ms\n",
      "Run 9600: 0.0233 ms\n",
      "Run 9700: 0.0288 ms\n",
      "Run 9800: 0.0227 ms\n",
      "Run 9900: 0.0233 ms\n",
      "Run 10000: 0.0231 ms\n",
      "\n",
      "Average inference time over 10000 runs: 0.0238 ms (std: 0.0314 ms)\n",
      "Average FPS over 10000 runs: 43218.7760 FPS (std: 2424.8054 FPS)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 4. Benchmark Inference Time\n",
    "# ---------------------------\n",
    "n_runs = 10000\n",
    "inference_times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time = time.perf_counter()\n",
    "    _ = session.run(None, {input_name: sample})\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Convert elapsed time from seconds to milliseconds\n",
    "    elapsed_ms = (end_time - start_time) * 1000\n",
    "    inference_times.append(elapsed_ms)\n",
    "    \n",
    "    # Print every 100 runs\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Run {i+1}: {elapsed_ms:.4f} ms\")\n",
    "\n",
    "# Compute mean and standard deviation of inference times (in ms)\n",
    "mean_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "print(f\"\\nAverage inference time over {n_runs} runs: {mean_time:.4f} ms (std: {std_time:.4f} ms)\")\n",
    "\n",
    "# Compute FPS for each run (FPS = 1 / time in seconds, or 1000 / time in ms)\n",
    "fps_values = 1000.0 / np.array(inference_times)\n",
    "mean_fps = np.mean(fps_values)\n",
    "std_fps = np.std(fps_values)\n",
    "print(f\"Average FPS over {n_runs} runs: {mean_fps:.4f} FPS (std: {std_fps:.4f} FPS)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef950a80-e940-48dd-9296-c64c76352868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
