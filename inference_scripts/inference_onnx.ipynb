{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53c07e1b-5fb8-43b4-9b67-d332a491ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8319cec-6ae0-471b-b6e4-aa3ee56327bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a2ef7b-b7d3-4cac-a05a-55e78f8ce00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set loaded from ./exported_test_set.csv\n",
      "X_test shape: (41992, 83)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Load the Exported Test Set\n",
    "# ---------------------------\n",
    "\n",
    "# Set the path to the exported test set CSV\n",
    "TEST_CSV = './exported_test_set.csv'\n",
    "\n",
    "if not os.path.exists(TEST_CSV):\n",
    "    raise FileNotFoundError(f\"Test set CSV not found at {TEST_CSV}\")\n",
    "\n",
    "# Load the test set from CSV\n",
    "X_test_export = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Assume the target column is named \"target\"\n",
    "Y_test = X_test_export[\"target\"].values\n",
    "X_test = X_test_export.drop(columns=[\"target\"])\n",
    "\n",
    "print(f\"Test set loaded from {TEST_CSV}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1a2939f-ef5e-4c78-ae23-0771e8d0b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_transformed shape: (41992, 94)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Define the Preprocessor\n",
    "# ---------------------------\n",
    "# Identify columns (you must know these from training)\n",
    "numerical_cols = X_test.select_dtypes(include=[\"int64\", \"float64\", \"float32\"]).columns.tolist()\n",
    "categorical_cols = [\"proto\", \"service\"]\n",
    "\n",
    "# Create new transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on test set (or a subset) and transform test data\n",
    "X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "print(f\"X_test_transformed shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fca89f-2ca6-40a7-b918-708e0789ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded successfully.\n",
      "ONNX model input name: input\n",
      "Expected input shape: [None, 94]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 3. Load ONNX Model and Inference \n",
    "# ---------------------------\n",
    "# Path to the saved ONNX model\n",
    "onnx_model_path = \"./onnx_models/best_svm_rbf_only_model_100.onnx\"\n",
    "# onnx_model_path = \"./onnx_models/best_rf_model_100.onnx\"\n",
    "# onnx_model_path = \"./onnx_models/best_lgb_model_100.onnx\"\n",
    "# onnx_model_path = \"./onnx_models/best_xgb_model_100.onnx\"\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    raise FileNotFoundError(f\"ONNX model not found at {onnx_model_path}\")\n",
    "\n",
    "# Create an ONNX Runtime inference session\n",
    "session = rt.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "input_name = session.get_inputs()[0].name\n",
    "print(\"ONNX model loaded successfully.\")\n",
    "print(\"ONNX model input name:\", input_name)\n",
    "print(\"Expected input shape:\", session.get_inputs()[0].shape)\n",
    "\n",
    "# Ensure the test set is in float32 (if not already)\n",
    "X_test_processed = X_test_transformed.astype(np.float32)\n",
    "\n",
    "# Choose a sample index (modify as needed)\n",
    "sample_index = 5  # for example, first sample\n",
    "sample = X_test_processed[sample_index:sample_index+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a666924c-7259-4bfe-9a70-103ad6bd5a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Single Sample Inference ---\n",
      "Sample index: 5\n",
      "True label: 3\n",
      "Predicted label: 3\n",
      "Predicted probabilities: {0: 1.0614944301323703e-07, 1: 3.1574006698065205e-07, 2: 5.36890567559567e-08, 3: 0.999686062335968, 4: 5.21222609677352e-05, 5: 8.729959517950192e-06, 6: 5.687565590051236e-06, 7: 3.141007255180739e-05, 8: 1.2431363757059444e-05, 9: 2.134592432412319e-05, 10: 1.2179171790194232e-05, 11: 5.623592369374819e-05, 12: 0.00011320886551402509}\n",
      "Accuracy (single sample): 1\n",
      "F1 Score (single sample): 1.0000\n",
      "Precision (single sample): 1.0000\n",
      "Recall (single sample): 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 5. Single Sample Inference and Metrics\n",
    "# ---------------------------\n",
    "\n",
    "# Choose a sample index (modify as needed)\n",
    "sample_index = 5  # for example, first sample\n",
    "\n",
    "onnx_outputs = session.run(None, {input_name: sample})\n",
    "# Assume the first output is the predicted label and the second (if exists) is the probabilities.\n",
    "predicted_label = onnx_outputs[0]\n",
    "predicted_probs = onnx_outputs[1] if len(onnx_outputs) > 1 else None\n",
    "\n",
    "# Print inference results for the sample\n",
    "true_label = Y_test[sample_index]\n",
    "print(\"\\n--- Single Sample Inference ---\")\n",
    "print(f\"Sample index: {sample_index}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label[0]}\")\n",
    "if predicted_probs is not None:\n",
    "    print(\"Predicted probabilities:\", predicted_probs[0])\n",
    "\n",
    "# For a single sample, accuracy is 1 if prediction matches, else 0.\n",
    "single_accuracy = 1 if predicted_label[0] == true_label else 0\n",
    "print(f\"Accuracy (single sample): {single_accuracy}\")\n",
    "\n",
    "# Compute additional metrics for the single sample (will be 0 or 1)\n",
    "y_true_single = np.array([true_label])\n",
    "y_pred_single = np.array([predicted_label[0]])\n",
    "\n",
    "f1 = f1_score(y_true_single, y_pred_single, average='macro')\n",
    "precision = precision_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"F1 Score (single sample): {f1:.4f}\")\n",
    "print(f\"Precision (single sample): {precision:.4f}\")\n",
    "print(f\"Recall (single sample): {recall:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Clean Up\n",
    "# ---------------------------\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf23a0f4-4fd6-42a3-a83b-e725c70194bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 100: 0.3279 ms\n",
      "Run 200: 0.3231 ms\n",
      "Run 300: 0.3222 ms\n",
      "Run 400: 0.2153 ms\n",
      "Run 500: 0.2146 ms\n",
      "Run 600: 0.2149 ms\n",
      "Run 700: 0.2150 ms\n",
      "Run 800: 0.2145 ms\n",
      "Run 900: 0.2162 ms\n",
      "Run 1000: 0.2148 ms\n",
      "Run 1100: 0.2193 ms\n",
      "Run 1200: 0.2148 ms\n",
      "Run 1300: 0.2150 ms\n",
      "Run 1400: 0.2151 ms\n",
      "Run 1500: 0.2148 ms\n",
      "Run 1600: 0.2152 ms\n",
      "Run 1700: 0.2146 ms\n",
      "Run 1800: 0.2200 ms\n",
      "Run 1900: 0.2151 ms\n",
      "Run 2000: 0.2227 ms\n",
      "Run 2100: 0.2153 ms\n",
      "Run 2200: 0.2145 ms\n",
      "Run 2300: 0.2151 ms\n",
      "Run 2400: 0.2150 ms\n",
      "Run 2500: 0.2157 ms\n",
      "Run 2600: 0.2151 ms\n",
      "Run 2700: 0.2202 ms\n",
      "Run 2800: 0.2146 ms\n",
      "Run 2900: 0.2148 ms\n",
      "Run 3000: 0.2150 ms\n",
      "Run 3100: 0.2152 ms\n",
      "Run 3200: 0.2154 ms\n",
      "Run 3300: 0.2150 ms\n",
      "Run 3400: 0.2163 ms\n",
      "Run 3500: 0.2145 ms\n",
      "Run 3600: 0.2234 ms\n",
      "Run 3700: 0.2149 ms\n",
      "Run 3800: 0.2150 ms\n",
      "Run 3900: 0.2153 ms\n",
      "Run 4000: 0.2148 ms\n",
      "Run 4100: 0.2147 ms\n",
      "Run 4200: 0.2153 ms\n",
      "Run 4300: 0.2222 ms\n",
      "Run 4400: 0.2150 ms\n",
      "Run 4500: 0.2148 ms\n",
      "Run 4600: 0.2151 ms\n",
      "Run 4700: 0.2149 ms\n",
      "Run 4800: 0.2151 ms\n",
      "Run 4900: 0.2146 ms\n",
      "Run 5000: 0.2157 ms\n",
      "Run 5100: 0.2147 ms\n",
      "Run 5200: 0.2237 ms\n",
      "Run 5300: 0.2144 ms\n",
      "Run 5400: 0.2145 ms\n",
      "Run 5500: 0.2149 ms\n",
      "Run 5600: 0.2145 ms\n",
      "Run 5700: 0.2149 ms\n",
      "Run 5800: 0.2152 ms\n",
      "Run 5900: 0.2236 ms\n",
      "Run 6000: 0.2151 ms\n",
      "Run 6100: 0.2238 ms\n",
      "Run 6200: 0.2147 ms\n",
      "Run 6300: 0.2149 ms\n",
      "Run 6400: 0.2149 ms\n",
      "Run 6500: 0.2149 ms\n",
      "Run 6600: 0.2165 ms\n",
      "Run 6700: 0.2150 ms\n",
      "Run 6800: 0.2244 ms\n",
      "Run 6900: 0.2151 ms\n",
      "Run 7000: 0.2149 ms\n",
      "Run 7100: 0.2146 ms\n",
      "Run 7200: 0.2149 ms\n",
      "Run 7300: 0.2150 ms\n",
      "Run 7400: 0.2148 ms\n",
      "Run 7500: 0.2152 ms\n",
      "Run 7600: 0.2143 ms\n",
      "Run 7700: 0.2242 ms\n",
      "Run 7800: 0.2151 ms\n",
      "Run 7900: 0.2144 ms\n",
      "Run 8000: 0.2152 ms\n",
      "Run 8100: 0.2149 ms\n",
      "Run 8200: 0.2153 ms\n",
      "Run 8300: 0.2145 ms\n",
      "Run 8400: 0.2174 ms\n",
      "Run 8500: 0.2150 ms\n",
      "Run 8600: 0.2266 ms\n",
      "Run 8700: 0.2149 ms\n",
      "Run 8800: 0.2152 ms\n",
      "Run 8900: 0.2157 ms\n",
      "Run 9000: 0.2151 ms\n",
      "Run 9100: 0.2149 ms\n",
      "Run 9200: 0.2149 ms\n",
      "Run 9300: 0.2158 ms\n",
      "Run 9400: 0.2149 ms\n",
      "Run 9500: 0.2274 ms\n",
      "Run 9600: 0.2145 ms\n",
      "Run 9700: 0.2149 ms\n",
      "Run 9800: 0.2150 ms\n",
      "Run 9900: 0.2150 ms\n",
      "Run 10000: 0.2156 ms\n",
      "\n",
      "Average inference time over 10000 runs: 0.2203 ms (std: 0.0214 ms)\n",
      "Average FPS over 10000 runs: 4568.8215 FPS (std: 298.7599 FPS)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 4. Benchmark Inference Time\n",
    "# ---------------------------\n",
    "n_runs = 10000\n",
    "inference_times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time = time.perf_counter()\n",
    "    _ = session.run(None, {input_name: sample})\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Convert elapsed time from seconds to milliseconds\n",
    "    elapsed_ms = (end_time - start_time) * 1000\n",
    "    inference_times.append(elapsed_ms)\n",
    "    \n",
    "    # Print every 100 runs\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Run {i+1}: {elapsed_ms:.4f} ms\")\n",
    "\n",
    "# Compute mean and standard deviation of inference times (in ms)\n",
    "mean_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "print(f\"\\nAverage inference time over {n_runs} runs: {mean_time:.4f} ms (std: {std_time:.4f} ms)\")\n",
    "\n",
    "# Compute FPS for each run (FPS = 1 / time in seconds, or 1000 / time in ms)\n",
    "fps_values = 1000.0 / np.array(inference_times)\n",
    "mean_fps = np.mean(fps_values)\n",
    "std_fps = np.std(fps_values)\n",
    "print(f\"Average FPS over {n_runs} runs: {mean_fps:.4f} FPS (std: {std_fps:.4f} FPS)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef950a80-e940-48dd-9296-c64c76352868",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
