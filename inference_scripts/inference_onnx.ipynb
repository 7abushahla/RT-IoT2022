{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53c07e1b-5fb8-43b4-9b67-d332a491ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import onnxruntime as rt\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a2ef7b-b7d3-4cac-a05a-55e78f8ce00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set loaded from ./exported_test_set.csv\n",
      "X_test shape: (41992, 83)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Load the Exported Test Set\n",
    "# ---------------------------\n",
    "\n",
    "# Set the path to the exported test set CSV\n",
    "TEST_CSV = './exported_test_set.csv'\n",
    "\n",
    "if not os.path.exists(TEST_CSV):\n",
    "    raise FileNotFoundError(f\"Test set CSV not found at {TEST_CSV}\")\n",
    "\n",
    "# Load the test set from CSV\n",
    "X_test_export = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Assume the target column is named \"target\"\n",
    "Y_test = X_test_export[\"target\"].values\n",
    "X_test = X_test_export.drop(columns=[\"target\"])\n",
    "\n",
    "print(f\"Test set loaded from {TEST_CSV}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1a2939f-ef5e-4c78-ae23-0771e8d0b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_transformed shape: (41992, 94)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Define the Preprocessor\n",
    "# ---------------------------\n",
    "# Identify columns (you must know these from training)\n",
    "numerical_cols = X_test.select_dtypes(include=[\"int64\", \"float64\", \"float32\"]).columns.tolist()\n",
    "categorical_cols = [\"proto\", \"service\"]\n",
    "\n",
    "# Create new transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on test set (or a subset) and transform test data\n",
    "X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "print(f\"X_test_transformed shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09fca89f-2ca6-40a7-b918-708e0789ec6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX model loaded successfully.\n",
      "ONNX model input name: input\n",
      "Expected input shape: [None, 94]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 3. Load ONNX Model and Inference \n",
    "# ---------------------------\n",
    "# Path to the saved ONNX model\n",
    "# onnx_model_path = \"./onnx_models/svm_classifier.onnx\"\n",
    "onnx_model_path = \"./onnx_models/best_rf_model.onnx\"\n",
    "# onnx_model_path = \"./onnx_models/best_lgb_model.onnx\"\n",
    "# onnx_model_path = \"./onnx_models/best_xgb_model_reg.onnx\"\n",
    "if not os.path.exists(onnx_model_path):\n",
    "    raise FileNotFoundError(f\"ONNX model not found at {onnx_model_path}\")\n",
    "\n",
    "# Create an ONNX Runtime inference session\n",
    "session = rt.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "input_name = session.get_inputs()[0].name\n",
    "print(\"ONNX model loaded successfully.\")\n",
    "print(\"ONNX model input name:\", input_name)\n",
    "print(\"Expected input shape:\", session.get_inputs()[0].shape)\n",
    "\n",
    "# Ensure the test set is in float32 (if not already)\n",
    "X_test_processed = X_test_transformed.astype(np.float32)\n",
    "\n",
    "# Choose a sample index (modify as needed)\n",
    "sample_index = 5  # for example, first sample\n",
    "sample = X_test_processed[sample_index:sample_index+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a666924c-7259-4bfe-9a70-103ad6bd5a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Single Sample Inference ---\n",
      "Sample index: 5\n",
      "True label: 3\n",
      "Predicted label: 3\n",
      "Predicted probabilities: {0: 0.09488033503293991, 1: 0.12566666305065155, 2: 0.0, 3: 0.4116666316986084, 4: 0.015833333134651184, 5: 0.004999999888241291, 6: 0.009999999776482582, 7: 0.1008671298623085, 8: 0.0801328718662262, 9: 0.025427350774407387, 10: 0.01733333431184292, 11: 0.04735897481441498, 12: 0.06583333760499954}\n",
      "Accuracy (single sample): 1\n",
      "F1 Score (single sample): 1.0000\n",
      "Precision (single sample): 1.0000\n",
      "Recall (single sample): 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 5. Single Sample Inference and Metrics\n",
    "# ---------------------------\n",
    "\n",
    "# Choose a sample index (modify as needed)\n",
    "sample_index = 5  # for example, first sample\n",
    "\n",
    "onnx_outputs = session.run(None, {input_name: sample})\n",
    "# Assume the first output is the predicted label and the second (if exists) is the probabilities.\n",
    "predicted_label = onnx_outputs[0]\n",
    "predicted_probs = onnx_outputs[1] if len(onnx_outputs) > 1 else None\n",
    "\n",
    "# Print inference results for the sample\n",
    "true_label = Y_test[sample_index]\n",
    "print(\"\\n--- Single Sample Inference ---\")\n",
    "print(f\"Sample index: {sample_index}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label[0]}\")\n",
    "if predicted_probs is not None:\n",
    "    print(\"Predicted probabilities:\", predicted_probs[0])\n",
    "\n",
    "# For a single sample, accuracy is 1 if prediction matches, else 0.\n",
    "single_accuracy = 1 if predicted_label[0] == true_label else 0\n",
    "print(f\"Accuracy (single sample): {single_accuracy}\")\n",
    "\n",
    "# Compute additional metrics for the single sample (will be 0 or 1)\n",
    "y_true_single = np.array([true_label])\n",
    "y_pred_single = np.array([predicted_label[0]])\n",
    "\n",
    "f1 = f1_score(y_true_single, y_pred_single, average='macro')\n",
    "precision = precision_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"F1 Score (single sample): {f1:.4f}\")\n",
    "print(f\"Precision (single sample): {precision:.4f}\")\n",
    "print(f\"Recall (single sample): {recall:.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Clean Up\n",
    "# ---------------------------\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf23a0f4-4fd6-42a3-a83b-e725c70194bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 100: 0.017 ms\n",
      "Run 200: 0.016 ms\n",
      "Run 300: 0.016 ms\n",
      "Run 400: 0.015 ms\n",
      "Run 500: 0.015 ms\n",
      "Run 600: 0.015 ms\n",
      "Run 700: 0.015 ms\n",
      "Run 800: 0.015 ms\n",
      "Run 900: 0.015 ms\n",
      "Run 1000: 0.015 ms\n",
      "\n",
      "Average inference time over 1000 runs: 0.016 ms (std: 0.008 ms)\n",
      "Average FPS over 1000 runs: 64739.341 FPS (std: 6540.985 FPS)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 4. Benchmark Inference Time\n",
    "# ---------------------------\n",
    "n_runs = 1000\n",
    "inference_times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time = time.perf_counter()\n",
    "    _ = session.run(None, {input_name: sample})\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Convert elapsed time from seconds to milliseconds\n",
    "    elapsed_ms = (end_time - start_time) * 1000\n",
    "    inference_times.append(elapsed_ms)\n",
    "    \n",
    "    # Print every 100 runs\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Run {i+1}: {elapsed_ms:.3f} ms\")\n",
    "\n",
    "# Compute mean and standard deviation of inference times (in ms)\n",
    "mean_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "print(f\"\\nAverage inference time over {n_runs} runs: {mean_time:.3f} ms (std: {std_time:.3f} ms)\")\n",
    "\n",
    "# Compute FPS for each run (FPS = 1 / time in seconds, or 1000 / time in ms)\n",
    "fps_values = 1000.0 / np.array(inference_times)\n",
    "mean_fps = np.mean(fps_values)\n",
    "std_fps = np.std(fps_values)\n",
    "print(f\"Average FPS over {n_runs} runs: {mean_fps:.3f} FPS (std: {std_fps:.3f} FPS)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
