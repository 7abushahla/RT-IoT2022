{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcfc91a-e59c-4351-81f2-1b7cfd8d7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e4a293-1407-4534-8255-659ecee7ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set loaded from ./exported_test_set.csv\n",
      "X_test shape: (41992, 83)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Load the Exported Test Set\n",
    "# ---------------------------\n",
    "\n",
    "# Set the path to the exported test set CSV\n",
    "TEST_CSV = './exported_test_set.csv'\n",
    "\n",
    "if not os.path.exists(TEST_CSV):\n",
    "    raise FileNotFoundError(f\"Test set CSV not found at {TEST_CSV}\")\n",
    "\n",
    "# Load the test set from CSV\n",
    "X_test_export = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Assume the target column is named \"target\"\n",
    "Y_test = X_test_export[\"target\"].values\n",
    "X_test = X_test_export.drop(columns=[\"target\"])\n",
    "\n",
    "print(f\"Test set loaded from {TEST_CSV}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f948fe-ce70-407d-a03b-afabccc96717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_transformed shape: (41992, 94)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Define the Preprocessor\n",
    "# ---------------------------\n",
    "# Identify columns (you must know these from training)\n",
    "numerical_cols = X_test.select_dtypes(include=[\"int64\", \"float64\", \"float32\"]).columns.tolist()\n",
    "categorical_cols = [\"proto\", \"service\"]\n",
    "\n",
    "# Create new transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on test set (or a subset) and transform test data\n",
    "X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "print(f\"X_test_transformed shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24c60cc-69c1-410d-9439-336b500f6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details with Quantization Parameters:\n",
      "  Name: serving_default_input_1:0\n",
      "  Shape: [ 1 94]\n",
      "  Data Type: <class 'numpy.float32'>\n",
      "  Quantization Parameters: (0.0, 0)\n",
      "  Quantization Scale: []\n",
      "  Quantization Zero Points: []\n",
      "\n",
      "\n",
      "Output Details with Quantization Parameters:\n",
      "  Name: StatefulPartitionedCall:0\n",
      "  Shape: [ 1 13]\n",
      "  Data Type: <class 'numpy.float32'>\n",
      "  Quantization Parameters: (0.0, 0)\n",
      "  Quantization Scale: []\n",
      "  Quantization Zero Points: []\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Load TFLite Float32 Model and Prepare for Inference\n",
    "# ---------------------------\n",
    "tflite_model_path = \"./tflite_models/best_FCNN_model.tflite\"  # update this path as needed\n",
    "if not os.path.exists(tflite_model_path):\n",
    "    raise FileNotFoundError(f\"TFLite model not found at {tflite_model_path}\")\n",
    "\n",
    "interpreter = tflite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Display Input Details\n",
    "print(\"Input Details with Quantization Parameters:\")\n",
    "for input_detail in input_details:\n",
    "    print(f\"  Name: {input_detail['name']}\")\n",
    "    print(f\"  Shape: {input_detail['shape']}\")\n",
    "    print(f\"  Data Type: {input_detail['dtype']}\")\n",
    "    print(f\"  Quantization Parameters: {input_detail['quantization']}\")\n",
    "    print(f\"  Quantization Scale: {input_detail['quantization_parameters']['scales']}\")\n",
    "    print(f\"  Quantization Zero Points: {input_detail['quantization_parameters']['zero_points']}\\n\")\n",
    "\n",
    "# Display Output Details\n",
    "print(\"\\nOutput Details with Quantization Parameters:\")\n",
    "for output_detail in output_details:\n",
    "    print(f\"  Name: {output_detail['name']}\")\n",
    "    print(f\"  Shape: {output_detail['shape']}\")\n",
    "    print(f\"  Data Type: {output_detail['dtype']}\")\n",
    "    print(f\"  Quantization Parameters: {output_detail['quantization']}\")\n",
    "    print(f\"  Quantization Scale: {output_detail['quantization_parameters']['scales']}\")\n",
    "    print(f\"  Quantization Zero Points: {output_detail['quantization_parameters']['zero_points']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "740962ac-daea-478e-abb7-7dea06c92b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Single Sample Inference (Float32) ---\n",
      "Sample index: 5\n",
      "True label: 3\n",
      "Predicted label: 3\n",
      "Accuracy (single sample): 1\n",
      "F1 Score (single sample): 1.0000\n",
      "Precision (single sample): 1.0000\n",
      "Recall (single sample): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 3. Single Sample Inference and Metrics (Float32)\n",
    "# ---------------------------\n",
    "X_test_processed = X_test_transformed.astype(np.float32)\n",
    "\n",
    "# Choose a sample index (modify as needed)\n",
    "sample_index = 5  # for example, first sample\n",
    "sample = X_test_processed[sample_index:sample_index+1]\n",
    "\n",
    "# Run single inference\n",
    "interpreter.set_tensor(input_details[0]['index'], sample)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "# Use argmax to select the predicted class from raw output scores (assumed shape is [1, num_classes])\n",
    "predicted_label = np.argmax(output_data, axis=1)\n",
    "true_label = Y_test[sample_index]\n",
    "\n",
    "print(\"\\n--- Single Sample Inference (Float32) ---\")\n",
    "print(f\"Sample index: {sample_index}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label[0]}\")\n",
    "\n",
    "# For a single sample, accuracy is 1 if the prediction matches, else 0.\n",
    "single_accuracy = 1 if predicted_label[0] == true_label else 0\n",
    "print(f\"Accuracy (single sample): {single_accuracy}\")\n",
    "\n",
    "# Compute additional metrics (these will be either 0 or 1)\n",
    "y_true_single = np.array([true_label])\n",
    "y_pred_single = np.array([predicted_label[0]])\n",
    "\n",
    "f1 = f1_score(y_true_single, y_pred_single, average='macro')\n",
    "precision = precision_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"F1 Score (single sample): {f1:.4f}\")\n",
    "print(f\"Precision (single sample): {precision:.4f}\")\n",
    "print(f\"Recall (single sample): {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "896fbebc-1a5c-4ed9-aabb-5189ca9e1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 100: 0.002 ms\n",
      "Run 200: 0.002 ms\n",
      "Run 300: 0.002 ms\n",
      "Run 400: 0.002 ms\n",
      "Run 500: 0.002 ms\n",
      "Run 600: 0.002 ms\n",
      "Run 700: 0.002 ms\n",
      "Run 800: 0.002 ms\n",
      "Run 900: 0.002 ms\n",
      "Run 1000: 0.002 ms\n",
      "\n",
      "Average inference time over 1000 runs: 0.003 ms (std: 0.008 ms)\n",
      "Average FPS over 1000 runs: 458856.612 FPS (std: 62489.876 FPS)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 4. Benchmark Inference Time and Compute FPS (Float32)\n",
    "# ---------------------------\n",
    "n_runs = 1000\n",
    "inference_times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time = time.perf_counter()\n",
    "    interpreter.set_tensor(input_details[0]['index'], sample)\n",
    "    interpreter.invoke()\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    elapsed_ms = (end_time - start_time) * 1000  # convert to milliseconds\n",
    "    inference_times.append(elapsed_ms)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Run {i+1}: {elapsed_ms:.3f} ms\")\n",
    "\n",
    "mean_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "fps_values = 1000.0 / np.array(inference_times)\n",
    "mean_fps = np.mean(fps_values)\n",
    "std_fps = np.std(fps_values)\n",
    "\n",
    "print(f\"\\nAverage inference time over {n_runs} runs: {mean_time:.3f} ms (std: {std_time:.3f} ms)\")\n",
    "print(f\"Average FPS over {n_runs} runs: {mean_fps:.3f} FPS (std: {std_fps:.3f} FPS)\")\n",
    "\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
