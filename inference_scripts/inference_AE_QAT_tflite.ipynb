{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcfc91a-e59c-4351-81f2-1b7cfd8d7211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tflite_runtime.interpreter as tflite\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31e4a293-1407-4534-8255-659ecee7ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set loaded from ./exported_test_set.csv\n",
      "X_test shape: (41992, 83)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 1. Load the Exported Test Set\n",
    "# ---------------------------\n",
    "\n",
    "# Set the path to the exported test set CSV\n",
    "TEST_CSV = './exported_test_set.csv'\n",
    "\n",
    "if not os.path.exists(TEST_CSV):\n",
    "    raise FileNotFoundError(f\"Test set CSV not found at {TEST_CSV}\")\n",
    "\n",
    "# Load the test set from CSV\n",
    "X_test_export = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Assume the target column is named \"target\"\n",
    "Y_test = X_test_export[\"target\"].values\n",
    "X_test = X_test_export.drop(columns=[\"target\"])\n",
    "\n",
    "print(f\"Test set loaded from {TEST_CSV}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f948fe-ce70-407d-a03b-afabccc96717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_transformed shape: (41992, 94)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Define the Preprocessor\n",
    "# ---------------------------\n",
    "# Identify columns (you must know these from training)\n",
    "numerical_cols = X_test.select_dtypes(include=[\"int64\", \"float64\", \"float32\"]).columns.tolist()\n",
    "categorical_cols = [\"proto\", \"service\"]\n",
    "\n",
    "# Create new transformers\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on test set (or a subset) and transform test data\n",
    "X_test_transformed = preprocessor.fit_transform(X_test)\n",
    "print(f\"X_test_transformed shape: {X_test_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a24c60cc-69c1-410d-9439-336b500f6c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Details with Quantization Parameters:\n",
      "  Name: serving_default_encoder_input:0\n",
      "  Shape: [ 1 94]\n",
      "  Data Type: <class 'numpy.int8'>\n",
      "  Quantization Parameters: (1.2719604969024658, -125)\n",
      "  Quantization Scale: [1.2719605]\n",
      "  Quantization Zero Points: [-125]\n",
      "\n",
      "\n",
      "Output Details with Quantization Parameters:\n",
      "Output 0:\n",
      "  Name: StatefulPartitionedCall:1\n",
      "  Shape: [ 1 94]\n",
      "  Data Type: <class 'numpy.int8'>\n",
      "  Quantization Parameters: (0.06721114367246628, -128)\n",
      "  Quantization Scale: [0.06721114]\n",
      "  Quantization Zero Points: [-128]\n",
      "\n",
      "Output 1:\n",
      "  Name: StatefulPartitionedCall:0\n",
      "  Shape: [ 1 13]\n",
      "  Data Type: <class 'numpy.int8'>\n",
      "  Quantization Parameters: (0.00390625, -128)\n",
      "  Quantization Scale: [0.00390625]\n",
      "  Quantization Zero Points: [-128]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 2. Load TFLite Float32 Model and Prepare for Inference\n",
    "# ---------------------------\n",
    "tflite_model_path = \"./tflite_models/best_AE_QAT_model.tflite\"  # update this path as needed\n",
    "if not os.path.exists(tflite_model_path):\n",
    "    raise FileNotFoundError(f\"TFLite model not found at {tflite_model_path}\")\n",
    "\n",
    "interpreter = tflite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Display Input Details\n",
    "print(\"Input Details with Quantization Parameters:\")\n",
    "for input_detail in input_details:\n",
    "    print(f\"  Name: {input_detail['name']}\")\n",
    "    print(f\"  Shape: {input_detail['shape']}\")\n",
    "    print(f\"  Data Type: {input_detail['dtype']}\")\n",
    "    print(f\"  Quantization Parameters: {input_detail['quantization']}\")\n",
    "    print(f\"  Quantization Scale: {input_detail['quantization_parameters']['scales']}\")\n",
    "    print(f\"  Quantization Zero Points: {input_detail['quantization_parameters']['zero_points']}\\n\")\n",
    "\n",
    "# Display Output Details\n",
    "print(\"\\nOutput Details with Quantization Parameters:\")\n",
    "for idx, output_detail in enumerate(output_details):\n",
    "    print(f\"Output {idx}:\")\n",
    "    print(f\"  Name: {output_detail['name']}\")\n",
    "    print(f\"  Shape: {output_detail['shape']}\")\n",
    "    print(f\"  Data Type: {output_detail['dtype']}\")\n",
    "    print(f\"  Quantization Parameters: {output_detail['quantization']}\")\n",
    "    print(f\"  Quantization Scale: {output_detail['quantization_parameters']['scales']}\")\n",
    "    print(f\"  Quantization Zero Points: {output_detail['quantization_parameters']['zero_points']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1419d9bc-3116-433c-8eb1-6b75b8774476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input quantization: scale = 1.2719604969024658, zero_point = -125\n",
      "Output quantization: scale = 0.00390625, zero_point = -128\n"
     ]
    }
   ],
   "source": [
    "# Retrieve quantization parameters\n",
    "input_scale, input_zero_point = input_details[0]['quantization']\n",
    "output_scale, output_zero_point = output_details[1]['quantization']\n",
    "\n",
    "print(f\"Input quantization: scale = {input_scale}, zero_point = {input_zero_point}\")\n",
    "print(f\"Output quantization: scale = {output_scale}, zero_point = {output_zero_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc28527-3c5c-49fa-a9f4-3bcb3e5ce66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Define Helper Functions for Quantization\n",
    "# ---------------------------\n",
    "def quantize_input(data, scale, zero_point):\n",
    "    \"\"\"\n",
    "    Quantizes a float32 input array to int8 using the provided scale and zero point.\n",
    "    \"\"\"\n",
    "    return np.round(data / scale + zero_point).astype(np.int8)\n",
    "\n",
    "def dequantize_output(data_int8, scale, zero_point):\n",
    "    \"\"\"\n",
    "    Converts quantized int8 output back to float32.\n",
    "    \"\"\"\n",
    "    return (data_int8.astype(np.float32) - zero_point) * scale\n",
    "\n",
    "def run_inference_tflite(input_data):\n",
    "    \"\"\"\n",
    "    Run inference on the TFLite model using pre-quantized input data and return the raw output.\n",
    "    \n",
    "    This function assumes that `input_data` is already pre-quantized (if the model is quantized)\n",
    "    and does not perform any dequantization on the output.\n",
    "    \"\"\"\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "740962ac-daea-478e-abb7-7dea06c92b60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using output index 1 for classification predictions.\n",
      "\n",
      "--- Single Sample Inference (Int8 Quantized for AE) ---\n",
      "Sample index: 5\n",
      "True label: 3\n",
      "Predicted label: 3\n",
      "Raw predicted scores (dequantized): [[0.         0.         0.         0.8828125  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.11328125\n",
      "  0.        ]]\n",
      "Accuracy (single sample): 1\n",
      "F1 Score (single sample): 1.0000\n",
      "Precision (single sample): 1.0000\n",
      "Recall (single sample): 1.0000\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 3. Single Sample Inference and Metrics (Int8 Quantized for AE)\n",
    "# ---------------------------\n",
    "X_test_processed = X_test_transformed.astype(np.float32)\n",
    "\n",
    "# Choose a sample index (modify as needed)\n",
    "sample_index = 5  # for example, first sample\n",
    "sample = X_test_processed[sample_index:sample_index+1]\n",
    "\n",
    "# Quantize the sample using the model's input quantization parameters.\n",
    "# The quantize_input function is assumed to be defined as:\n",
    "# def quantize_input(data, scale, zero_point):\n",
    "#     return np.round(data / scale + zero_point).astype(np.int8)\n",
    "sample_quantized = quantize_input(sample, input_scale, input_zero_point)\n",
    "\n",
    "# Run inference: set the input tensor with the pre-quantized sample and invoke.\n",
    "interpreter.set_tensor(input_details[0]['index'], sample_quantized)\n",
    "interpreter.invoke()\n",
    "\n",
    "# Use output index 1 for classification predictions.\n",
    "print(\"Using output index 1 for classification predictions.\")\n",
    "predicted_output = interpreter.get_tensor(output_details[1]['index'])\n",
    "\n",
    "# Dequantize the output using the provided dequantize_output function:\n",
    "# def dequantize_output(data_int8, scale, zero_point):\n",
    "#     return (data_int8.astype(np.float32) - zero_point) * scale\n",
    "output_scale, output_zero_point = output_details[1]['quantization']\n",
    "predicted_output_dequant = dequantize_output(predicted_output, output_scale, output_zero_point)\n",
    "\n",
    "# Convert raw output scores to predicted class by taking argmax.\n",
    "predicted_label = np.argmax(predicted_output_dequant, axis=1)\n",
    "true_label = Y_test[sample_index]\n",
    "\n",
    "print(\"\\n--- Single Sample Inference (Int8 Quantized for AE) ---\")\n",
    "print(f\"Sample index: {sample_index}\")\n",
    "print(f\"True label: {true_label}\")\n",
    "print(f\"Predicted label: {predicted_label[0]}\")\n",
    "print(\"Raw predicted scores (dequantized):\", predicted_output_dequant)\n",
    "\n",
    "# For a single sample, accuracy is 1 if the prediction matches the true label, else 0.\n",
    "single_accuracy = 1 if predicted_label[0] == true_label else 0\n",
    "print(f\"Accuracy (single sample): {single_accuracy}\")\n",
    "\n",
    "# Compute additional metrics for the single sample (they will be either 0 or 1)\n",
    "y_true_single = np.array([true_label])\n",
    "y_pred_single = np.array([predicted_label[0]])\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "f1 = f1_score(y_true_single, y_pred_single, average='macro')\n",
    "precision = precision_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "recall = recall_score(y_true_single, y_pred_single, average='macro', zero_division=0)\n",
    "\n",
    "print(f\"F1 Score (single sample): {f1:.4f}\")\n",
    "print(f\"Precision (single sample): {precision:.4f}\")\n",
    "print(f\"Recall (single sample): {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "896fbebc-1a5c-4ed9-aabb-5189ca9e1583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 100: 0.010 ms\n",
      "Run 200: 0.010 ms\n",
      "Run 300: 0.010 ms\n",
      "Run 400: 0.010 ms\n",
      "Run 500: 0.010 ms\n",
      "Run 600: 0.010 ms\n",
      "Run 700: 0.009 ms\n",
      "Run 800: 0.010 ms\n",
      "Run 900: 0.010 ms\n",
      "Run 1000: 0.010 ms\n",
      "\n",
      "Average inference time over 1000 runs: 0.011 ms (std: 0.007 ms)\n",
      "Average FPS over 1000 runs: 100870.519 FPS (std: 14027.560 FPS)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 4. Benchmark Inference Time and Compute FPS (Int8 Quantized)\n",
    "# ---------------------------\n",
    "n_runs = 1000\n",
    "inference_times = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    start_time = time.perf_counter()\n",
    "    # Run inference using our run_inference_tflite function.\n",
    "    _ = run_inference_tflite(sample_quantized)  # sample_quantized is the pre-quantized sample.\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    # Convert elapsed time from seconds to milliseconds.\n",
    "    elapsed_ms = (end_time - start_time) * 1000\n",
    "    inference_times.append(elapsed_ms)\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"Run {i+1}: {elapsed_ms:.3f} ms\")\n",
    "\n",
    "mean_time = np.mean(inference_times)\n",
    "std_time = np.std(inference_times)\n",
    "fps_values = 1000.0 / np.array(inference_times)  # FPS = 1000 ms / inference time in ms.\n",
    "mean_fps = np.mean(fps_values)\n",
    "std_fps = np.std(fps_values)\n",
    "\n",
    "print(f\"\\nAverage inference time over {n_runs} runs: {mean_time:.3f} ms (std: {std_time:.3f} ms)\")\n",
    "print(f\"Average FPS over {n_runs} runs: {mean_fps:.3f} FPS (std: {std_fps:.3f} FPS)\")\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fac5ae-eb76-4c37-b625-bae16cd14901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
