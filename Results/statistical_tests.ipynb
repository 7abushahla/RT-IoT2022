{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------\n",
    "# Runs the statistical pipeline described in the paper:\n",
    "# - Q1: Friedman (repeated measures) + Wilcoxon post-hoc (Holm)\n",
    "# - Q2: FP32 vs INT8 (Shapiro -> paired t or Wilcoxon)\n",
    "# Saves CSV and tex tables; prints a succinct console summary.\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy.stats import shapiro, ttest_rel, wilcoxon, friedmanchisquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIG ==========\n",
    "CSV_PATH = \"./model_scores_per_fold.csv\"  # <-- change path if needed\n",
    "\n",
    "# Expected model display names (any order is fine)\n",
    "MODEL_ORDER = [\n",
    "    \"Random Forest\", \"SVM\", \"XGBoost\", \"LightGBM\",\n",
    "    \"FCNN (FP32)\", \"FCNN (INT8)\", \"AutoEncoder (FP32)\", \"AutoEncoder (INT8)\"\n",
    "]\n",
    "\n",
    "# Mapping from CSV column prefixes to display names\n",
    "PREFIX_TO_MODEL = {\n",
    "    \"RF\": \"Random Forest\",\n",
    "    \"SVM\": \"SVM\", \n",
    "    \"XGB\": \"XGBoost\",\n",
    "    \"LGB\": \"LightGBM\",\n",
    "    \"FCNN32\": \"FCNN (FP32)\",\n",
    "    \"FCNN8\": \"FCNN (INT8)\",\n",
    "    \"AE32\": \"AutoEncoder (FP32)\",\n",
    "    \"AE8\": \"AutoEncoder (INT8)\"\n",
    "}\n",
    "\n",
    "# Metric mapping from CSV suffixes to standard names\n",
    "METRIC_MAPPING = {\n",
    "    \"Acc\": \"Accuracy\",\n",
    "    \"BalAcc\": \"Balanced Accuracy\", \n",
    "    \"F1\": \"Macro F1\",\n",
    "    \"ROC\": \"ROC AUC\",\n",
    "    \"Prec\": \"Precision (Macro)\",\n",
    "    \"Rec\": \"Recall (Macro)\"\n",
    "}\n",
    "\n",
    "# Quantization pairs (architecture FP32 vs INT8)\n",
    "QUANT_PAIRS = [\n",
    "    (\"FCNN (FP32)\", \"FCNN (INT8)\"),\n",
    "    (\"AutoEncoder (FP32)\", \"AutoEncoder (INT8)\")\n",
    "]\n",
    "\n",
    "# Metric column names to analyze (will auto-detect if empty)\n",
    "METRICS = []  # e.g., [\"Accuracy\",\"Balanced Accuracy\",\"Macro F1\",\"ROC AUC\",\"Precision (Macro)\",\"Recall (Macro)\"]\n",
    "\n",
    "# Column names for LONG format (will auto-infer if None)\n",
    "MODEL_COL = None  # e.g., \"Model\"\n",
    "FOLD_COL  = None  # e.g., \"fold\" or \"fold_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Helpers ==========\n",
    "def holm_bonferroni(pvals_dict):\n",
    "    # dict[(a,b)] = p  -> returns adjusted p-values dict with same keys\n",
    "    items = sorted(pvals_dict.items(), key=lambda kv: kv[1])\n",
    "    m = len(items)\n",
    "    out = {}\n",
    "    for i, (k, p) in enumerate(items, start=1):\n",
    "        out[k] = min((m - i + 1) * p, 1.0)\n",
    "    return out\n",
    "\n",
    "def cohen_d_paired(a, b):\n",
    "    d = np.asarray(a) - np.asarray(b)\n",
    "    sd = d.std(ddof=1)\n",
    "    return d.mean() / (sd if sd > 0 else np.nan)\n",
    "\n",
    "def cliffs_delta(a, b):\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    gt = sum((x > y) for x in a for y in b)\n",
    "    lt = sum((x < y) for x in a for y in b)\n",
    "    n = len(a) * len(b)\n",
    "    return (gt - lt) / n if n else np.nan\n",
    "\n",
    "def write_latex(df, path, caption, label):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(df.to_latex(index=False, escape=True,\n",
    "                            caption=caption, label=label,\n",
    "                            float_format=\"%.4g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected format: WIDE\n",
      "Found metrics: ['Accuracy', 'Balanced Accuracy', 'Macro F1', 'Precision (Macro)', 'ROC AUC', 'Recall (Macro)']\n",
      "DataFrame shape: (240, 8)\n",
      "Columns: ['fold', 'Model', 'Accuracy', 'Balanced Accuracy', 'Macro F1', 'Precision (Macro)', 'ROC AUC', 'Recall (Macro)']\n",
      "Models found: ['AutoEncoder (FP32)', 'AutoEncoder (INT8)', 'FCNN (FP32)', 'FCNN (INT8)', 'LightGBM', 'Random Forest', 'SVM', 'XGBoost']\n",
      "Models to use: ['Random Forest', 'SVM', 'XGBoost', 'LightGBM', 'FCNN (FP32)', 'FCNN (INT8)', 'AutoEncoder (FP32)', 'AutoEncoder (INT8)']\n"
     ]
    }
   ],
   "source": [
    "# ========== Load CSV and detect shape ==========\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Try to detect LONG vs WIDE\n",
    "def is_long_format(d: pd.DataFrame):\n",
    "    nonnum = [c for c in d.columns if not np.issubdtype(d[c].dtype, np.number)]\n",
    "    return len(nonnum) >= 1 and any('model' in c.lower() for c in nonnum)\n",
    "\n",
    "long_format = is_long_format(df)\n",
    "print(f\"Detected format: {'LONG' if long_format else 'WIDE'}\")\n",
    "\n",
    "if long_format:\n",
    "    # Infer MODEL_COL and FOLD_COL if needed\n",
    "    if MODEL_COL is None:\n",
    "        nonnum = [c for c in df.columns if not np.issubdtype(df[c].dtype, np.number)]\n",
    "        MODEL_COL = nonnum[0]\n",
    "    if FOLD_COL is None:\n",
    "        candidates = [c for c in df.columns if \"fold\" in c.lower() or \"repeat\" in c.lower() or \"cv\" in c.lower()]\n",
    "        FOLD_COL = candidates[0] if candidates else None\n",
    "\n",
    "    # Build metric list if not provided\n",
    "    if not METRICS:\n",
    "        ignore = {MODEL_COL}\n",
    "        if FOLD_COL: ignore.add(FOLD_COL)\n",
    "        METRICS = [c for c in df.columns if c not in ignore and np.issubdtype(df[c].dtype, np.number)]\n",
    "\n",
    "    # Clean model labels\n",
    "    df[MODEL_COL] = df[MODEL_COL].astype(str).str.strip()\n",
    "else:\n",
    "    # WIDE format: create LONG view for processing.\n",
    "    # Parse columns like \"RF_Acc\", \"SVM_BalAcc\", etc.\n",
    "    \n",
    "    # Build metric list from column suffixes\n",
    "    if not METRICS:\n",
    "        suffixes = set()\n",
    "        for col in df.columns:\n",
    "            if '_' in col and col != 'Fold':\n",
    "                suffix = col.split('_', 1)[1]\n",
    "                suffixes.add(suffix)\n",
    "        METRICS = [METRIC_MAPPING.get(s, s) for s in sorted(suffixes)]\n",
    "    \n",
    "    print(f\"Found metrics: {METRICS}\")\n",
    "    \n",
    "    # Build LONG format by parsing column names\n",
    "    rows = []\n",
    "    for col in df.columns:\n",
    "        if col == 'Fold':\n",
    "            continue\n",
    "        if '_' in col:\n",
    "            prefix, suffix = col.split('_', 1)\n",
    "            if prefix in PREFIX_TO_MODEL:\n",
    "                model_name = PREFIX_TO_MODEL[prefix]\n",
    "                metric_name = METRIC_MAPPING.get(suffix, suffix)\n",
    "                \n",
    "                for idx, row in df.iterrows():\n",
    "                    rows.append({\n",
    "                        'fold': row['Fold'],\n",
    "                        'Model': model_name,\n",
    "                        'metric_name': metric_name,\n",
    "                        'value': row[col]\n",
    "                    })\n",
    "    \n",
    "    long_df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Pivot to (fold, model) x metrics\n",
    "    MODEL_COL, FOLD_COL = \"Model\", \"fold\"\n",
    "    df = long_df.pivot_table(index=[FOLD_COL, MODEL_COL], columns=\"metric_name\", values=\"value\").reset_index()\n",
    "    \n",
    "    # Clean up column names\n",
    "    df.columns.name = None\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Now we are in LONG-like form with MODEL_COL, FOLD_COL, METRICS\n",
    "models_present = sorted(df[MODEL_COL].unique())\n",
    "print(f\"Models found: {models_present}\")\n",
    "\n",
    "# Keep only models we know (if present)\n",
    "models_used = [m for m in MODEL_ORDER if m in models_present]\n",
    "print(f\"Models to use: {models_used}\")\n",
    "\n",
    "if len(models_used) < 3:\n",
    "    raise ValueError(f\"Fewer than 3 recognized models. Found: {models_present}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Accuracy...\n",
      "Data shape for Accuracy: (30, 8)\n",
      "\n",
      "Processing Balanced Accuracy...\n",
      "Data shape for Balanced Accuracy: (30, 8)\n",
      "\n",
      "Processing Macro F1...\n",
      "Data shape for Macro F1: (30, 8)\n",
      "\n",
      "Processing Precision (Macro)...\n",
      "Data shape for Precision (Macro): (30, 8)\n",
      "\n",
      "Processing ROC AUC...\n",
      "Data shape for ROC AUC: (30, 8)\n",
      "\n",
      "Processing Recall (Macro)...\n",
      "Data shape for Recall (Macro): (30, 8)\n",
      "\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Build wide matrices per metric: rows = folds, cols = models\n",
    "def metric_wide(metric):\n",
    "    w = (df[[FOLD_COL, MODEL_COL, metric]]\n",
    "         .pivot_table(index=FOLD_COL, columns=MODEL_COL, values=metric, aggfunc=\"first\"))\n",
    "    # keep only selected models and drop any rows with missing\n",
    "    w = w.reindex(columns=models_used)\n",
    "    w = w.dropna(axis=0, how=\"any\")\n",
    "    return w\n",
    "\n",
    "friedman_rows = []\n",
    "posthoc_rows  = []\n",
    "quant_rows    = []\n",
    "\n",
    "for metric in METRICS:\n",
    "    print(f\"\\nProcessing {metric}...\")\n",
    "    W = metric_wide(metric)\n",
    "    if W.empty or W.shape[0] < 3:\n",
    "        print(f\"Skipping {metric} - insufficient data\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Data shape for {metric}: {W.shape}\")\n",
    "\n",
    "    # ---------- Q1: Friedman ----------\n",
    "    try:\n",
    "        fr_stat, fr_p = friedmanchisquare(*[W[m].values for m in W.columns])\n",
    "    except Exception as e:\n",
    "        print(f\"Friedman test failed for {metric}: {e}\")\n",
    "        fr_stat, fr_p = (np.nan, np.nan)\n",
    "    friedman_rows.append({\n",
    "        \"metric\": metric,\n",
    "        \"n_folds\": W.shape[0],\n",
    "        \"n_models\": W.shape[1],\n",
    "        \"chi2\": fr_stat,\n",
    "        \"p_value\": fr_p\n",
    "    })\n",
    "\n",
    "    # Post-hoc Wilcoxon (paired) with Holm; include model1/model2 + effect\n",
    "    pair_p = {}\n",
    "    pair_stat = {}\n",
    "    pair_eff = {}\n",
    "\n",
    "    for m1, m2 in combinations(W.columns, 2):\n",
    "        s1, s2 = W[m1].values, W[m2].values\n",
    "        try:\n",
    "            stat, p = wilcoxon(s1, s2, zero_method='wilcox', alternative='two-sided')\n",
    "        except ValueError:\n",
    "            stat, p = (0.0, 1.0)\n",
    "        pair_p[(m1, m2)] = p\n",
    "        pair_stat[(m1, m2)] = stat\n",
    "        # Effect size for post-hoc: Cliff's delta (direction m1 vs m2)\n",
    "        pair_eff[(m1, m2)] = cliffs_delta(s1, s2)\n",
    "\n",
    "    adj = holm_bonferroni(pair_p)\n",
    "    for (m1, m2), p in pair_p.items():\n",
    "        posthoc_rows.append({\n",
    "            \"metric\": metric,\n",
    "            \"model1\": m1,\n",
    "            \"model2\": m2,\n",
    "            \"pair\": f\"{m1} vs {m2}\",\n",
    "            \"wilcoxon_stat\": pair_stat[(m1, m2)],\n",
    "            \"p_raw\": p,\n",
    "            \"p_holm\": adj[(m1, m2)],\n",
    "            \"effect\": pair_eff[(m1, m2)]\n",
    "        })\n",
    "\n",
    "    # ---------- Q2: Quantization (per architecture) ----------\n",
    "    for a, b in QUANT_PAIRS:\n",
    "        if a in W.columns and b in W.columns:\n",
    "            diffs = W[a].values - W[b].values\n",
    "            p_norm = np.nan\n",
    "            test_name = \"Wilcoxon\"\n",
    "            stat = np.nan\n",
    "            pval = np.nan\n",
    "            eff = np.nan\n",
    "            if diffs.size >= 3 and diffs.var() > 0:\n",
    "                try:\n",
    "                    _, p_norm = shapiro(diffs)\n",
    "                except Exception:\n",
    "                    p_norm = np.nan\n",
    "                if not np.isnan(p_norm) and p_norm > 0.05:\n",
    "                    # paired t\n",
    "                    stat, pval = ttest_rel(W[a].values, W[b].values)\n",
    "                    eff = cohen_d_paired(W[a].values, W[b].values)\n",
    "                    test_name = \"Paired t\"\n",
    "                else:\n",
    "                    # Wilcoxon\n",
    "                    try:\n",
    "                        stat, pval = wilcoxon(W[a].values, W[b].values, zero_method='wilcox', alternative='two-sided')\n",
    "                    except ValueError:\n",
    "                        stat, pval = (0.0, 1.0)\n",
    "                    eff = cliffs_delta(W[a].values, W[b].values)\n",
    "            quant_rows.append({\n",
    "                \"metric\": metric,\n",
    "                \"pair\": f\"{a} vs {b}\",\n",
    "                \"n_folds\": W.shape[0],\n",
    "                \"normality_p\": p_norm,\n",
    "                \"test\": test_name,\n",
    "                \"stat\": stat,\n",
    "                \"p_value\": pval,\n",
    "                \"effect\": eff\n",
    "            })\n",
    "\n",
    "print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to TXT files:\n",
      "- friedman_results.txt\n",
      "- posthoc_wilcoxon_holm.txt\n",
      "- quantization_tests.txt\n",
      "Saved: q1_summary.txt\n",
      "\n",
      "======================================================================\n",
      "SUMMARY\n",
      "======================================================================\n",
      "Models analyzed: 8\n",
      "Metrics analyzed: 6\n",
      "\n",
      "[Q1] WHICH MODEL IS BEST? (Friedman + Ranks + Post-hoc)\n",
      "Friedman significant (p < 0.05): 6 / 6 metrics\n",
      "  - Accuracy: χ²=206.611, p=4.56e-41\n",
      "  - Balanced Accuracy: χ²=181.322, p=1.02e-35\n",
      "  - Macro F1: χ²=181.889, p=7.77e-36\n",
      "  - Precision (Macro): χ²=163.567, p=5.69e-32\n",
      "  - ROC AUC: χ²=173.122, p=5.51e-34\n",
      "  - Recall (Macro): χ²=181.322, p=1.02e-35\n",
      "\n",
      "Global leaderboard (average rank across metrics):\n",
      "  XGBoost: avg rank=1.95, Wins=35, Losses=4\n",
      "  LightGBM: avg rank=2.64, Wins=30, Losses=7\n",
      "  Random Forest: avg rank=2.81, Wins=31, Losses=10\n",
      "  AutoEncoder (FP32): avg rank=4.48, Wins=17, Losses=15\n",
      "  AutoEncoder (INT8): avg rank=5.27, Wins=12, Losses=21\n",
      "  FCNN (FP32): avg rank=5.39, Wins=10, Losses=22\n",
      "  FCNN (INT8): avg rank=5.45, Wins=7, Losses=21\n",
      "  SVM: avg rank=8.00, Wins=0, Losses=42\n",
      "\n",
      "Per-metric best model (mean ± sd; avg rank):\n",
      "  - Accuracy: XGBoost (0.9988 ± 0.0001; rank=1.00)\n",
      "  - Balanced Accuracy: XGBoost (0.9787 ± 0.0032; rank=1.00)\n",
      "  - Macro F1: Random Forest (0.9807 ± 0.0026; rank=1.50)\n",
      "  - Precision (Macro): LightGBM (0.9961 ± 0.0010; rank=1.30)\n",
      "  - ROC AUC: XGBoost (1.0000 ± 0.0000; rank=1.00)\n",
      "  - Recall (Macro): XGBoost (0.9787 ± 0.0032; rank=1.00)\n",
      "\n",
      "Top post-hoc (directional, Holm-corrected p) — smallest first:\n",
      "  Accuracy: AutoEncoder (FP32) > AutoEncoder (INT8) (p_holm=5.59e-09)\n",
      "  Accuracy: FCNN (INT8) < AutoEncoder (FP32) (p_holm=7.45e-09)\n",
      "  Accuracy: FCNN (FP32) > AutoEncoder (INT8) (p_holm=9.31e-09)\n",
      "  Accuracy: FCNN (FP32) > FCNN (INT8) (p_holm=1.12e-08)\n",
      "  Accuracy: LightGBM > AutoEncoder (INT8) (p_holm=1.30e-08)\n",
      "  Accuracy: LightGBM > AutoEncoder (FP32) (p_holm=1.49e-08)\n",
      "  Accuracy: LightGBM > FCNN (INT8) (p_holm=1.68e-08)\n",
      "  Accuracy: LightGBM > FCNN (FP32) (p_holm=1.86e-08)\n",
      "  Accuracy: XGBoost > AutoEncoder (INT8) (p_holm=2.05e-08)\n",
      "  ROC AUC: XGBoost > AutoEncoder (INT8) (p_holm=2.05e-08)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Q2] DOES INT8 QUANTIZATION AFFECT PERFORMANCE?\n",
      "Significant quantization differences (p < 0.05): 7 cases\n",
      "  - Accuracy [AutoEncoder (FP32) vs AutoEncoder (INT8)]: Wilcoxon, p=1.863e-09, effect=0.963\n",
      "  - Accuracy [FCNN (FP32) vs FCNN (INT8)]: Paired t, p=2.15e-12, effect=2.114\n",
      "  - Balanced Accuracy [FCNN (FP32) vs FCNN (INT8)]: Wilcoxon, p=0.04491, effect=-0.318\n",
      "  - Macro F1 [AutoEncoder (FP32) vs AutoEncoder (INT8)]: Paired t, p=0.0001137, effect=0.814\n",
      "  - Precision (Macro) [AutoEncoder (FP32) vs AutoEncoder (INT8)]: Paired t, p=4.358e-07, effect=1.182\n",
      "  - ROC AUC [AutoEncoder (FP32) vs AutoEncoder (INT8)]: Wilcoxon, p=0.03643, effect=-0.436\n",
      "  - Recall (Macro) [FCNN (FP32) vs FCNN (INT8)]: Wilcoxon, p=0.04491, effect=-0.318\n",
      "Non-significant quantization results: 5 cases\n",
      "  - Balanced Accuracy [AutoEncoder (FP32) vs AutoEncoder (INT8)]: Paired t, p=0.8209, effect=0.042\n",
      "  - Macro F1 [FCNN (FP32) vs FCNN (INT8)]: Wilcoxon, p=0.2367, effect=-0.184\n",
      "  - Precision (Macro) [FCNN (FP32) vs FCNN (INT8)]: Wilcoxon, p=0.5291, effect=0.118\n",
      "  - ROC AUC [FCNN (FP32) vs FCNN (INT8)]: Paired t, p=0.4709, effect=0.133\n",
      "  - Recall (Macro) [AutoEncoder (FP32) vs AutoEncoder (INT8)]: Paired t, p=0.8209, effect=0.042\n"
     ]
    }
   ],
   "source": [
    "# ---------- Save as .TXT (human-readable) + ENHANCED Q1 SUMMARY ----------\n",
    "\n",
    "def save_txt(df, path, title):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"{title}\\n\")\n",
    "        f.write(\"=\" * len(title) + \"\\n\")\n",
    "        if df.empty:\n",
    "            f.write(\"(no results)\\n\")\n",
    "        else:\n",
    "            f.write(df.to_string(index=False))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "# Create DataFrames (sorted)\n",
    "friedman_df = pd.DataFrame(friedman_rows).sort_values([\"metric\"])\n",
    "posthoc_df  = pd.DataFrame(posthoc_rows).sort_values([\"metric\",\"p_holm\"])\n",
    "quant_df    = pd.DataFrame(quant_rows).sort_values([\"metric\",\"pair\"])\n",
    "\n",
    "# Save to .txt files\n",
    "save_txt(friedman_df, \"friedman_results.txt\",\n",
    "         \"Friedman omnibus tests across models (per metric)\")\n",
    "save_txt(posthoc_df,  \"posthoc_wilcoxon_holm.txt\",\n",
    "         \"Post-hoc Wilcoxon pairs after Holm correction\")\n",
    "save_txt(quant_df,    \"quantization_tests.txt\",\n",
    "         \"Quantization comparisons (FP32 vs INT8) with chosen test and effect size\")\n",
    "\n",
    "print(\"Results saved to TXT files:\")\n",
    "print(\"- friedman_results.txt\")\n",
    "print(\"- posthoc_wilcoxon_holm.txt\")\n",
    "print(\"- quantization_tests.txt\")\n",
    "\n",
    "# ---------------------- Q1: build richer summary ----------------------\n",
    "\n",
    "# Ensure posthoc has model1/model2/effect; if not, derive\n",
    "if not posthoc_df.empty:\n",
    "    if \"model1\" not in posthoc_df.columns or \"model2\" not in posthoc_df.columns:\n",
    "        split = posthoc_df[\"pair\"].str.split(\" vs \", n=1, expand=True)\n",
    "        posthoc_df[\"model1\"] = split[0]\n",
    "        posthoc_df[\"model2\"] = split[1]\n",
    "    if \"effect\" not in posthoc_df.columns:\n",
    "        # compute Cliff's Δ as an effect size direction for each pair/metric\n",
    "        def _pair_effect(r):\n",
    "            Wloc = metric_wide(r[\"metric\"])\n",
    "            m1, m2 = r[\"model1\"], r[\"model2\"]\n",
    "            if m1 in Wloc.columns and m2 in Wloc.columns:\n",
    "                return cliffs_delta(Wloc[m1].values, Wloc[m2].values)\n",
    "            return np.nan\n",
    "        posthoc_df[\"effect\"] = posthoc_df.apply(_pair_effect, axis=1)\n",
    "\n",
    "# Compute per-metric means/SDs and average ranks; build global leaderboard\n",
    "per_metric_summaries = []\n",
    "global_rank_accum = []\n",
    "\n",
    "for metric in METRICS:\n",
    "    Wm = metric_wide(metric)\n",
    "    if Wm.empty:\n",
    "        continue\n",
    "    means = Wm.mean(axis=0)\n",
    "    stds  = Wm.std(axis=0, ddof=1)\n",
    "    ranks = Wm.rank(axis=1, method=\"average\", ascending=False)   # higher is better\n",
    "    avg_ranks = ranks.mean(axis=0).sort_values()                 # ↓ is better\n",
    "    global_rank_accum.append(avg_ranks)\n",
    "\n",
    "    metric_df = pd.DataFrame({\n",
    "        \"Metric\": metric,\n",
    "        \"Model\": means.index,\n",
    "        \"Mean\": means.values,\n",
    "        \"SD\": stds.reindex(means.index).values,\n",
    "        \"Avg Rank\": avg_ranks.reindex(means.index).values\n",
    "    }).sort_values([\"Avg Rank\",\"Mean\"], ascending=[True, False])\n",
    "    per_metric_summaries.append(metric_df)\n",
    "\n",
    "if global_rank_accum:\n",
    "    global_rank = pd.concat(global_rank_accum, axis=1).mean(axis=1).sort_values()\n",
    "    # Directional wins/losses from significant post-hoc (Holm < .05)\n",
    "    posthoc_sig = posthoc_df[posthoc_df[\"p_holm\"] < 0.05].copy()\n",
    "    # get median diff to determine direction (m1 > m2 or vice versa)\n",
    "    def _median_diff(r):\n",
    "        Wloc = metric_wide(r[\"metric\"])\n",
    "        if r[\"model1\"] in Wloc.columns and r[\"model2\"] in Wloc.columns:\n",
    "            d = (Wloc[r[\"model1\"]] - Wloc[r[\"model2\"]]).values\n",
    "            return np.median(d) if len(d) else np.nan\n",
    "        return np.nan\n",
    "    if not posthoc_sig.empty:\n",
    "        posthoc_sig[\"median_diff\"] = posthoc_sig.apply(_median_diff, axis=1)\n",
    "    win_counts = {m: 0 for m in models_used}\n",
    "    loss_counts = {m: 0 for m in models_used}\n",
    "    for _, r in (posthoc_sig if not posthoc_sig.empty else pd.DataFrame()).iterrows():\n",
    "        if pd.notnull(r[\"median_diff\"]):\n",
    "            if r[\"median_diff\"] > 0:\n",
    "                win_counts[r[\"model1\"]] += 1\n",
    "                loss_counts[r[\"model2\"]] += 1\n",
    "            elif r[\"median_diff\"] < 0:\n",
    "                win_counts[r[\"model2\"]] += 1\n",
    "                loss_counts[r[\"model1\"]] += 1\n",
    "    global_table = pd.DataFrame({\n",
    "        \"Model\": global_rank.index,\n",
    "        \"Global Avg Rank (↓ better)\": global_rank.values,\n",
    "        \"Wins (Holm<0.05)\": [win_counts.get(m,0) for m in global_rank.index],\n",
    "        \"Losses\": [loss_counts.get(m,0) for m in global_rank.index]\n",
    "    })\n",
    "else:\n",
    "    posthoc_sig = pd.DataFrame()\n",
    "    global_table = pd.DataFrame()\n",
    "\n",
    "# Save Q1 human-readable summary file\n",
    "with open(\"q1_summary.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Q1 SUMMARY: Which model is best?\\n\")\n",
    "    f.write(\"================================\\n\\n\")\n",
    "    f.write(\"Friedman omnibus per metric:\\n\")\n",
    "    if friedman_df.empty:\n",
    "        f.write(\"  (no results)\\n\\n\")\n",
    "    else:\n",
    "        for _, row in friedman_df.iterrows():\n",
    "            f.write(f\"  - {row['metric']}: chi2={row['chi2']:.3f}, p={row['p_value']:.2e}\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"Global leaderboard (average rank across metrics):\\n\")\n",
    "    if global_table.empty:\n",
    "        f.write(\"  (no leaderboard)\\n\\n\")\n",
    "    else:\n",
    "        f.write(global_table.to_string(index=False))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "    f.write(\"Per-metric best model (mean ± sd; avg rank):\\n\")\n",
    "    if not per_metric_summaries:\n",
    "        f.write(\"  (none)\\n\")\n",
    "    else:\n",
    "        for metric_df in per_metric_summaries:\n",
    "            top = metric_df.iloc[0]\n",
    "            f.write(f\"  - {top['Metric']}: {top['Model']} \"\n",
    "                    f\"({top['Mean']:.4f} ± {top['SD']:.4f}; rank={top['Avg Rank']:.2f})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "    f.write(\"Significant post-hoc pairs after Holm (directional):\\n\")\n",
    "    if posthoc_sig.empty:\n",
    "        f.write(\"  (none)\\n\")\n",
    "    else:\n",
    "        for _, r in posthoc_sig.sort_values([\"p_holm\",\"metric\"]).head(20).iterrows():\n",
    "            arrow = \" > \" if r[\"median_diff\"] > 0 else (\" < \" if r[\"median_diff\"] < 0 else \" = \")\n",
    "            f.write(f\"  - {r['metric']}: {r['model1']}{arrow}{r['model2']} \"\n",
    "                    f\"(p_holm={r['p_holm']:.2e})\\n\")\n",
    "print(\"Saved: q1_summary.txt\")\n",
    "\n",
    "# ========================= Console SUMMARY (enhanced) =========================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Models analyzed: {len(models_used)}\")\n",
    "print(f\"Metrics analyzed: {len(METRICS)}\")\n",
    "\n",
    "# ---------- Q1: Model comparison ----------\n",
    "print(\"\\n[Q1] WHICH MODEL IS BEST? (Friedman + Ranks + Post-hoc)\")\n",
    "sig_friedman = friedman_df[friedman_df['p_value'] < 0.05]\n",
    "nonsig_friedman = friedman_df[friedman_df['p_value'] >= 0.05]\n",
    "\n",
    "if not sig_friedman.empty:\n",
    "    print(f\"Friedman significant (p < 0.05): {len(sig_friedman)} / {len(friedman_df)} metrics\")\n",
    "    for _, row in sig_friedman.iterrows():\n",
    "        print(f\"  - {row['metric']}: χ²={row['chi2']:.3f}, p={row['p_value']:.2e}\")\n",
    "else:\n",
    "    print(\"No metrics reached significance in Friedman test.\")\n",
    "\n",
    "if not nonsig_friedman.empty:\n",
    "    print(f\"Friedman non-significant: {len(nonsig_friedman)} metrics\")\n",
    "    for _, row in nonsig_friedman.iterrows():\n",
    "        print(f\"  - {row['metric']}: χ²={row['chi2']:.3f}, p={row['p_value']:.2e}\")\n",
    "\n",
    "# Global leaderboard\n",
    "if not global_table.empty:\n",
    "    print(\"\\nGlobal leaderboard (average rank across metrics):\")\n",
    "    for _, r in global_table.sort_values(\"Global Avg Rank (↓ better)\").iterrows():\n",
    "        print(f\"  {r['Model']}: avg rank={r['Global Avg Rank (↓ better)']:.2f}, \"\n",
    "              f\"Wins={int(r['Wins (Holm<0.05)'])}, Losses={int(r['Losses'])}\")\n",
    "\n",
    "# Per-metric bests\n",
    "if per_metric_summaries:\n",
    "    print(\"\\nPer-metric best model (mean ± sd; avg rank):\")\n",
    "    for metric_df in per_metric_summaries:\n",
    "        top = metric_df.iloc[0]\n",
    "        print(f\"  - {top['Metric']}: {top['Model']} \"\n",
    "              f\"({top['Mean']:.4f} ± {top['SD']:.4f}; rank={top['Avg Rank']:.2f})\")\n",
    "\n",
    "# Post-hoc directional highlights\n",
    "if not posthoc_sig.empty:\n",
    "    print(\"\\nTop post-hoc (directional, Holm-corrected p) — smallest first:\")\n",
    "    for _, r in posthoc_sig.sort_values([\"p_holm\",\"metric\"]).head(10).iterrows():\n",
    "        arrow = \" > \" if r[\"median_diff\"] > 0 else (\" < \" if r[\"median_diff\"] < 0 else \" = \"\n",
    "        )\n",
    "        print(f\"  {r['metric']}: {r['model1']}{arrow}{r['model2']} (p_holm={r['p_holm']:.2e})\")\n",
    "else:\n",
    "    print(\"\\nNo significant post-hoc pairs after Holm.\")\n",
    "\n",
    "# ---------- Separator ----------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# ---------- Q2: Quantization effect ----------\n",
    "print(\"[Q2] DOES INT8 QUANTIZATION AFFECT PERFORMANCE?\")\n",
    "sig_quant = quant_df[quant_df['p_value'] < 0.05]\n",
    "nonsig_quant = quant_df[quant_df['p_value'] >= 0.05]\n",
    "\n",
    "if not sig_quant.empty:\n",
    "    print(f\"Significant quantization differences (p < 0.05): {len(sig_quant)} cases\")\n",
    "    for _, row in sig_quant.iterrows():\n",
    "        print(f\"  - {row['metric']} [{row['pair']}]: {row['test']}, \"\n",
    "              f\"p={row['p_value']:.4g}, effect={row['effect']:.3f}\")\n",
    "else:\n",
    "    print(\"No significant quantization effects found.\")\n",
    "\n",
    "if not nonsig_quant.empty:\n",
    "    print(f\"Non-significant quantization results: {len(nonsig_quant)} cases\")\n",
    "    for _, row in nonsig_quant.iterrows():\n",
    "        print(f\"  - {row['metric']} [{row['pair']}]: {row['test']}, \"\n",
    "              f\"p={row['p_value']:.4g}, effect={row['effect']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Q1] WHICH MODEL IS BEST? (Friedman + Ranks + Post-hoc)\n",
      "Friedman test: Significant differences detected in 6/6 metrics (p < 0.05).\n",
      "\n",
      "Per-metric significance:\n",
      "  ✔ Accuracy: χ²=206.611, p=4.563e-41\n",
      "  ✔ Balanced Accuracy: χ²=181.322, p=1.024e-35\n",
      "  ✔ Macro F1: χ²=181.889, p=7.774e-36\n",
      "  ✔ Precision (Macro): χ²=163.567, p=5.693e-32\n",
      "  ✔ ROC AUC: χ²=173.122, p=5.512e-34\n",
      "  ✔ Recall (Macro): χ²=181.322, p=1.024e-35\n",
      "\n",
      "Global Leaderboard (avg rank across all metrics):\n",
      "  XGBoost: rank=1.95, Wins=35, Losses=4\n",
      "  LightGBM: rank=2.64, Wins=30, Losses=7\n",
      "  Random Forest: rank=2.81, Wins=31, Losses=10\n",
      "  AutoEncoder (FP32): rank=4.48, Wins=17, Losses=15\n",
      "  AutoEncoder (INT8): rank=5.27, Wins=12, Losses=21\n",
      "  FCNN (FP32): rank=5.39, Wins=10, Losses=22\n",
      "  FCNN (INT8): rank=5.45, Wins=7, Losses=21\n",
      "  SVM: rank=8.00, Wins=0, Losses=42\n",
      "\n",
      "Best model per metric (mean ± sd; avg rank):\n",
      "  - Accuracy: XGBoost (0.9988 ± 0.0001; rank=1.00)\n",
      "  - Balanced Accuracy: XGBoost (0.9787 ± 0.0032; rank=1.00)\n",
      "  - Macro F1: Random Forest (0.9807 ± 0.0026; rank=1.50)\n",
      "  - Precision (Macro): LightGBM (0.9961 ± 0.0010; rank=1.30)\n",
      "  - ROC AUC: XGBoost (1.0000 ± 0.0000; rank=1.00)\n",
      "  - Recall (Macro): XGBoost (0.9787 ± 0.0032; rank=1.00)\n",
      "\n",
      "Top Post-hoc Comparisons (Holm-corrected p, smallest first):\n",
      "  Accuracy: AutoEncoder (FP32) > AutoEncoder (INT8) (p=5.59e-09, effect=0.963)\n",
      "  Accuracy: AutoEncoder (FP32) > FCNN (INT8) (p=7.45e-09, effect=0.981)\n",
      "  Accuracy: FCNN (FP32) > AutoEncoder (INT8) (p=9.31e-09, effect=0.964)\n",
      "  Accuracy: FCNN (FP32) > FCNN (INT8) (p=1.12e-08, effect=0.976)\n",
      "  Accuracy: LightGBM > AutoEncoder (INT8) (p=1.30e-08, effect=1.000)\n",
      "  Accuracy: LightGBM > AutoEncoder (FP32) (p=1.49e-08, effect=1.000)\n",
      "  Accuracy: LightGBM > FCNN (INT8) (p=1.68e-08, effect=1.000)\n",
      "  Accuracy: LightGBM > FCNN (FP32) (p=1.86e-08, effect=1.000)\n",
      "  Accuracy: XGBoost > AutoEncoder (INT8) (p=2.05e-08, effect=1.000)\n",
      "  ROC AUC: XGBoost > AutoEncoder (INT8) (p=2.05e-08, effect=1.000)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "[Q2] DOES INT8 QUANTIZATION AFFECT PERFORMANCE?\n",
      "Quantization effect detected in 7/12 cases.\n",
      "\n",
      "FCNN (FP32 vs INT8):\n",
      "  ✔ Accuracy: Paired t, p=2.15e-12, effect=2.114\n",
      "  ✔ Balanced Accuracy: Wilcoxon, p=0.04491, effect=-0.318\n",
      "  ✘ Macro F1: Wilcoxon, p=0.2367, effect=-0.184\n",
      "  ✘ Precision (Macro): Wilcoxon, p=0.5291, effect=0.118\n",
      "  ✘ ROC AUC: Paired t, p=0.4709, effect=0.133\n",
      "  ✔ Recall (Macro): Wilcoxon, p=0.04491, effect=-0.318\n",
      "\n",
      "AutoEncoder (FP32 vs INT8):\n",
      "  ✔ Accuracy: Wilcoxon, p=1.863e-09, effect=0.963\n",
      "  ✘ Balanced Accuracy: Paired t, p=0.8209, effect=0.042\n",
      "  ✔ Macro F1: Paired t, p=0.0001137, effect=0.814\n",
      "  ✔ Precision (Macro): Paired t, p=4.358e-07, effect=1.182\n",
      "  ✔ ROC AUC: Wilcoxon, p=0.03643, effect=-0.436\n",
      "  ✘ Recall (Macro): Paired t, p=0.8209, effect=0.042\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Q1: WHICH MODEL IS BEST?\n",
    "# =============================\n",
    "print(\"\\n[Q1] WHICH MODEL IS BEST? (Friedman + Ranks + Post-hoc)\")\n",
    "\n",
    "# Friedman results summary\n",
    "sig_friedman = friedman_df[friedman_df['p_value'] < 0.05]\n",
    "nonsig_friedman = friedman_df[friedman_df['p_value'] >= 0.05]\n",
    "\n",
    "if not sig_friedman.empty:\n",
    "    print(f\"Friedman test: Significant differences detected in \"\n",
    "          f\"{len(sig_friedman)}/{len(friedman_df)} metrics (p < 0.05).\")\n",
    "else:\n",
    "    print(\"Friedman test: No significant differences between models.\")\n",
    "\n",
    "print(\"\\nPer-metric significance:\")\n",
    "for _, row in friedman_df.iterrows():\n",
    "    sig_flag = \"✔\" if row['p_value'] < 0.05 else \"✘\"\n",
    "    print(f\"  {sig_flag} {row['metric']}: χ²={row['chi2']:.3f}, p={row['p_value']:.4g}\")\n",
    "\n",
    "# Global leaderboard\n",
    "print(\"\\nGlobal Leaderboard (avg rank across all metrics):\")\n",
    "if not global_table.empty:\n",
    "    for _, r in global_table.sort_values(\"Global Avg Rank (↓ better)\").iterrows():\n",
    "        print(f\"  {r['Model']}: rank={r['Global Avg Rank (↓ better)']:.2f}, \"\n",
    "              f\"Wins={int(r['Wins (Holm<0.05)'])}, Losses={int(r['Losses'])}\")\n",
    "else:\n",
    "    print(\"  (no leaderboard)\")\n",
    "\n",
    "# Per-metric best model\n",
    "print(\"\\nBest model per metric (mean ± sd; avg rank):\")\n",
    "if per_metric_summaries:\n",
    "    for metric_df in per_metric_summaries:\n",
    "        top = metric_df.iloc[0]\n",
    "        print(f\"  - {top['Metric']}: {top['Model']} \"\n",
    "              f\"({top['Mean']:.4f} ± {top['SD']:.4f}; rank={top['Avg Rank']:.2f})\")\n",
    "else:\n",
    "    print(\"  (no per-metric bests)\")\n",
    "\n",
    "# Post-hoc top pairs (directional)\n",
    "print(\"\\nTop Post-hoc Comparisons (Holm-corrected p, smallest first):\")\n",
    "if 'posthoc_sig' in locals() and not posthoc_sig.empty:\n",
    "    show = posthoc_sig.sort_values([\"p_holm\", \"metric\"]).head(10)\n",
    "    for _, row in show.iterrows():\n",
    "        # Determine direction using median_diff (computed earlier)\n",
    "        if 'median_diff' in row and pd.notnull(row['median_diff']):\n",
    "            better = row['model1'] if row['median_diff'] > 0 else row['model2']\n",
    "            worse  = row['model2'] if row['median_diff'] > 0 else row['model1']\n",
    "        else:\n",
    "            # Fallback to effect sign if median_diff missing\n",
    "            better = row['model1'] if row.get('effect', 0) > 0 else row['model2']\n",
    "            worse  = row['model2'] if row.get('effect', 0) > 0 else row['model1']\n",
    "        eff_val = row['effect'] if 'effect' in row and pd.notnull(row['effect']) else np.nan\n",
    "        print(f\"  {row['metric']}: {better} > {worse} (p={row['p_holm']:.2e}, effect={abs(eff_val):.3f})\")\n",
    "else:\n",
    "    print(\"  (no significant post-hoc pairs)\")\n",
    "\n",
    "# ---------- Separator ----------\n",
    "print(\"\\n\" + \"-\"*70)\n",
    "\n",
    "# =============================\n",
    "# Q2: DOES INT8 QUANTIZATION AFFECT PERFORMANCE?\n",
    "# =============================\n",
    "print(\"[Q2] DOES INT8 QUANTIZATION AFFECT PERFORMANCE?\")\n",
    "\n",
    "sig_quant = quant_df[quant_df['p_value'] < 0.05]\n",
    "nonsig_quant = quant_df[quant_df['p_value'] >= 0.05]\n",
    "\n",
    "# Summary\n",
    "if not sig_quant.empty:\n",
    "    print(f\"Quantization effect detected in {len(sig_quant)}/{len(quant_df)} cases.\")\n",
    "else:\n",
    "    print(\"No significant quantization effects detected.\")\n",
    "\n",
    "# FCNN summary\n",
    "print(\"\\nFCNN (FP32 vs INT8):\")\n",
    "fcnn_results = quant_df[quant_df['pair'].str.contains(\"FCNN\")]\n",
    "if not fcnn_results.empty:\n",
    "    for _, row in fcnn_results.iterrows():\n",
    "        sig_flag = \"✔\" if row['p_value'] < 0.05 else \"✘\"\n",
    "        print(f\"  {sig_flag} {row['metric']}: {row['test']}, \"\n",
    "              f\"p={row['p_value']:.4g}, effect={row['effect']:.3f}\")\n",
    "else:\n",
    "    print(\"  (no FCNN comparison rows)\")\n",
    "\n",
    "# AutoEncoder summary\n",
    "print(\"\\nAutoEncoder (FP32 vs INT8):\")\n",
    "ae_results = quant_df[quant_df['pair'].str.contains(\"AutoEncoder\")]\n",
    "if not ae_results.empty:\n",
    "    for _, row in ae_results.iterrows():\n",
    "        sig_flag = \"✔\" if row['p_value'] < 0.05 else \"✘\"\n",
    "        print(f\"  {sig_flag} {row['metric']}: {row['test']}, \"\n",
    "              f\"p={row['p_value']:.4g}, effect={row['effect']:.3f}\")\n",
    "else:\n",
    "    print(\"  (no AutoEncoder comparison rows)\")\n",
    "\n",
    "# Overall interpretation\n",
    "# print(\"\\nInterpretation:\")\n",
    "# print(\"  - FCNN: Performance consistently drops with INT8 quantization (most or all metrics significant).\")\n",
    "# print(\"  - AutoEncoder: Mixed impact — typically Accuracy, Precision, and AUC show significant drops; \"\n",
    "#       \"Balanced Accuracy, Macro F1, and Recall often remain non-significant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
