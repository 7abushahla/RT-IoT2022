Q1 SUMMARY: Which model is best?
================================

Friedman omnibus per metric:
  - Accuracy: chi2=206.611, p=4.56e-41
  - Balanced Accuracy: chi2=181.322, p=1.02e-35
  - Macro F1: chi2=181.889, p=7.77e-36
  - Precision (Macro): chi2=163.567, p=5.69e-32
  - ROC AUC: chi2=173.122, p=5.51e-34
  - Recall (Macro): chi2=181.322, p=1.02e-35

Global leaderboard (average rank across metrics):
             Model  Global Avg Rank (↓ better)  Wins (Holm<0.05)  Losses
           XGBoost                    1.950000                35       4
          LightGBM                    2.644444                30       7
     Random Forest                    2.811111                31      10
AutoEncoder (FP32)                    4.483333                17      15
AutoEncoder (INT8)                    5.266667                12      21
       FCNN (FP32)                    5.394444                10      22
       FCNN (INT8)                    5.450000                 7      21
               SVM                    8.000000                 0      42

Per-metric best model (mean ± sd; avg rank):
  - Accuracy: XGBoost (0.9988 ± 0.0001; rank=1.00)
  - Balanced Accuracy: XGBoost (0.9787 ± 0.0032; rank=1.00)
  - Macro F1: Random Forest (0.9807 ± 0.0026; rank=1.50)
  - Precision (Macro): LightGBM (0.9961 ± 0.0010; rank=1.30)
  - ROC AUC: XGBoost (1.0000 ± 0.0000; rank=1.00)
  - Recall (Macro): XGBoost (0.9787 ± 0.0032; rank=1.00)

Significant post-hoc pairs after Holm (directional):
  - Accuracy: Random Forest > SVM (p_holm=5.22e-08)
  - Accuracy: Random Forest < XGBoost (p_holm=5.22e-08)
  - Accuracy: Random Forest > LightGBM (p_holm=5.22e-08)
  - Accuracy: Random Forest > FCNN (FP32) (p_holm=5.22e-08)
  - Accuracy: Random Forest > FCNN (INT8) (p_holm=5.22e-08)
  - Accuracy: Random Forest > AutoEncoder (FP32) (p_holm=5.22e-08)
  - Accuracy: Random Forest > AutoEncoder (INT8) (p_holm=5.22e-08)
  - Accuracy: SVM < XGBoost (p_holm=5.22e-08)
  - Accuracy: SVM < LightGBM (p_holm=5.22e-08)
  - Accuracy: SVM < FCNN (FP32) (p_holm=5.22e-08)
  - Accuracy: SVM < FCNN (INT8) (p_holm=5.22e-08)
  - Accuracy: SVM < AutoEncoder (FP32) (p_holm=5.22e-08)
  - Accuracy: SVM < AutoEncoder (INT8) (p_holm=5.22e-08)
  - Accuracy: XGBoost > LightGBM (p_holm=5.22e-08)
  - Accuracy: XGBoost > FCNN (FP32) (p_holm=5.22e-08)
  - Accuracy: XGBoost > FCNN (INT8) (p_holm=5.22e-08)
  - Accuracy: XGBoost > AutoEncoder (FP32) (p_holm=5.22e-08)
  - Accuracy: XGBoost > AutoEncoder (INT8) (p_holm=5.22e-08)
  - Accuracy: LightGBM > FCNN (FP32) (p_holm=5.22e-08)
  - Accuracy: LightGBM > FCNN (INT8) (p_holm=5.22e-08)

==================================================
SIGNIFICANCE LEVEL SENSITIVITY ANALYSIS
==================================================

Robustness test across multiple α levels:
  α =    0.1: 6/6 metrics significant
  α =   0.05: 6/6 metrics significant
  α =   0.01: 6/6 metrics significant

Conclusion: Results robust across significance levels.
Strong evidence with controlled Type I error risk.
No additional Type II errors introduced by conservative testing.
