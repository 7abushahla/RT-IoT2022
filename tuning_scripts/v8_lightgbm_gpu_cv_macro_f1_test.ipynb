{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc # garbage collection\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import early_stopping\n",
    "import lightgbm as lgb\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\b00088568\\AppData\\Local\\anaconda3\\envs\\optunasec_lightgbm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix TQDM Warning:\n",
    "# https://stackoverflow.com/questions/75349025/vs-code-jupyter-notebook-iprogress-not-found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect() # forces garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn.exceptions\n",
    "'''\n",
    "During Optuna's Hyperparameter Optimization, we get this error:\n",
    " UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in\n",
    " labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
    "\n",
    "Even with Stratified KFold, because we calculate precision like:\n",
    "True Positive / Predicted Yes\n",
    "\n",
    "And if we predicted all our samples to be No\n",
    "\n",
    "Then Predicted Yes = 0, resulting in undefined precision -- possibly undefined F-score\n",
    "Link:\n",
    "https://stackoverflow.com/questions/35225369/scikit-learn-error-message-precision-and-f-score-are-ill-defined-and-being-set\n",
    "'''\n",
    "# warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed at file level\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "FILE_NAME = 'LightGBM_Official_No_RFECV_No_PCA'\n",
    "N_FOLDS = 10\n",
    "CV_RESULT_DIR = \"./lightgbm_cv_results\"\n",
    "TUNED_OR_NO = \"Tuned_Hyperparameters\" # or Untuned_Hyperparameters\n",
    "N_REPEATS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directories are created for results\n",
    "os.makedirs(f\"./{FILE_NAME}/{TUNED_OR_NO}/{CV_RESULT_DIR}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "url = './RT_IOT2022_new.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['service'] = data['service'].replace('-', 'None')\n",
    "data['proto'] = data['proto'].replace('-', 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the target column\n",
    "target = data['Attack_type']\n",
    "data = data.drop(columns = ['Attack_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size= 0.2, random_state=SEED, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode the target variable\n",
    "# LE = LabelEncoder()\n",
    "# target_encoded = LE.fit_transform(target)\n",
    "# CLASS_LABELS = LE.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory-saving\n",
    "del data\n",
    "del target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Macro F1 score metric for multiclass classification\n",
    "def macro_f1_score(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    num_class = len(np.unique(y_true))\n",
    "    \n",
    "    # Validate the length of preds\n",
    "    if len(preds) == len(y_true):  # If preds are class indices\n",
    "        y_pred = preds\n",
    "    elif len(preds) == len(y_true) * num_class:  # If preds are probabilities\n",
    "        y_pred = np.argmax(preds.reshape(len(y_true), num_class), axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected prediction shape\")\n",
    "    # Reshape preds to (num_instances, num_class)\n",
    "    y_pred = np.argmax(preds.reshape(-1, num_class), axis=1)\n",
    "    \n",
    "    # Calculate Macro F1 Score\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    \n",
    "    return 'macro_f1', macro_f1, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=N_FOLDS, n_repeats=N_REPEATS, random_state=SEED)\n",
    "    \n",
    "    param = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"verbosity\": -1,\n",
    "        \"n_estimators\": 1_500,\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
    "        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"gbdt\", \"dart\", \"goss\"]),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 1_000),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 100),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-8, 1.0, log=True),\n",
    "        \"min_split_gain\": trial.suggest_float(\"min_split_gain\", 0, 15),\n",
    "        \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-3, 10.0),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 2, 300),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 10.0, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 10.0, log=True),\n",
    "        \"num_class\": len(np.unique(Y_train)),\n",
    "        \"device\": \"gpu\",\n",
    "        \"n_jobs\": 8,\n",
    "        \"seed\": SEED,\n",
    "        \"bagging_seed\": SEED,\n",
    "        \"metric\": \"None\"  # Disable the default metrics (e.g., multi_logloss)\n",
    "    }\n",
    "\n",
    "    # Handle incompatible parameters\n",
    "    if param[\"boosting_type\"] == \"goss\":\n",
    "        param[\"subsample\"] = 1.0\n",
    "        param[\"bagging_fraction\"] = 1.0\n",
    "        param[\"bagging_freq\"] = 0\n",
    "        \n",
    "    if param[\"boosting_type\"] == \"dart\":\n",
    "        param[\"drop_rate\"] = trial.suggest_float(\"drop_rate\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"max_drop\"] = trial.suggest_int(\"max_drop\", 1, 50)\n",
    "        \n",
    "    if param[\"boosting_type\"] != \"goss\":\n",
    "        param[\"feature_fraction\"] = trial.suggest_float(\"feature_fraction\", 0.1, 0.9)\n",
    "        param[\"bagging_fraction\"] = trial.suggest_float(\"bagging_fraction\", 0.1, 0.9)\n",
    "        param[\"bagging_freq\"] = trial.suggest_int(\"bagging_freq\", 1, 1_000)\n",
    "\n",
    "    # Additional parameters\n",
    "    param[\"max_bin\"] = trial.suggest_int(\"max_bin\", 10, 255)\n",
    "    param[\"cat_l2\"] = trial.suggest_float(\"cat_l2\", 1e-8, 10.0, log=True)\n",
    "    param[\"cat_smooth\"] = trial.suggest_float(\"cat_smooth\", 1e-8, 10.0, log=True)\n",
    "    param[\"feature_fraction_bynode\"] = trial.suggest_float(\"feature_fraction_bynode\", 0.1, 0.9)\n",
    "\n",
    "    trial_macro_f1_score = []\n",
    "\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(rskf.split(X_train, Y_train)):\n",
    "        fold_num = fold_idx + 1\n",
    "        \n",
    "        # Split the raw data into training and validation sets for this fold\n",
    "        X_train_fold = X_train.iloc[train_idx].copy()\n",
    "        Y_train_fold = Y_train.iloc[train_idx].copy()\n",
    "        X_valid_fold = X_train.iloc[valid_idx].copy()\n",
    "        Y_valid_fold = Y_train.iloc[valid_idx].copy()\n",
    "        \n",
    "        # Preprocessing within the fold\n",
    "        X_train_fold.reset_index(drop=True, inplace=True)\n",
    "        X_valid_fold.reset_index(drop=True, inplace=True)\n",
    "        Y_train_fold.reset_index(drop=True, inplace=True)\n",
    "        Y_valid_fold.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Standardizing numerical features\n",
    "        num_cols = X_train_fold.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "        scaler = StandardScaler()\n",
    "        X_train_fold[num_cols] = scaler.fit_transform(X_train_fold[num_cols])\n",
    "        X_valid_fold[num_cols] = scaler.transform(X_valid_fold[num_cols])\n",
    "        \n",
    "        # Encoding target variable (convert to integer labels)\n",
    "        Y_train_fold = Y_train_fold.astype('category').cat.codes\n",
    "        Y_valid_fold = Y_valid_fold.astype('category').cat.codes\n",
    "\n",
    "        # Identify categorical columns to encode\n",
    "        categorical_cols = ['proto', 'service']\n",
    "        \n",
    "        # Encoding categorical variables using OneHotEncoder\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        encoder.fit(X_train_fold[categorical_cols])\n",
    "        \n",
    "        # Transform both training and validation sets\n",
    "        X_train_encoded = encoder.transform(X_train_fold[categorical_cols])\n",
    "        X_valid_encoded = encoder.transform(X_valid_fold[categorical_cols])\n",
    "        \n",
    "        # Convert to DataFrame and reset index\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out())\n",
    "        X_valid_encoded_df = pd.DataFrame(X_valid_encoded, columns=encoder.get_feature_names_out())\n",
    "        \n",
    "        # Drop original categorical columns\n",
    "        X_train_fold = X_train_fold.drop(categorical_cols, axis=1)\n",
    "        X_valid_fold = X_valid_fold.drop(categorical_cols, axis=1)\n",
    "        \n",
    "        # Concatenate encoded features\n",
    "        X_train_fold = pd.concat([X_train_fold, X_train_encoded_df], axis=1)\n",
    "        X_valid_fold = pd.concat([X_valid_fold, X_valid_encoded_df], axis=1)\n",
    "\n",
    "        del X_train_encoded_df, X_valid_encoded_df\n",
    "\n",
    "        # Convert to LightGBM Dataset\n",
    "        dtrain = lgb.Dataset(X_train_fold, label=Y_train_fold)\n",
    "        dvalid = lgb.Dataset(X_valid_fold, label=Y_valid_fold, reference=dtrain)\n",
    "        \n",
    "        # Train the model using lgb.train\n",
    "        model = lgb.train(\n",
    "            param,\n",
    "            dtrain,\n",
    "            valid_sets=[dtrain, dvalid],\n",
    "            valid_names=['train', 'valid'],\n",
    "            feval=macro_f1_score,\n",
    "            callbacks=[early_stopping(stopping_rounds=300), lgb.log_evaluation(100, show_stdv=True)]\n",
    "        )\n",
    "\n",
    "        # Predict on validation data\n",
    "        y_pred = model.predict(X_valid_fold)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        del X_train_fold, X_valid_fold\n",
    "\n",
    "        # Compute metrics\n",
    "        macro_f1 = f1_score(Y_valid_fold, y_pred, average='macro')\n",
    "        trial_macro_f1_score.append(macro_f1)\n",
    "\n",
    "        del Y_train_fold, Y_valid_fold\n",
    "        \n",
    "        # Clean up\n",
    "        del model\n",
    "        gc.collect()\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    mean_macro_f1 = np.mean(trial_macro_f1_score)\n",
    "    trial.report(mean_macro_f1, step=N_FOLDS * N_REPEATS)\n",
    "\n",
    "    # Prune trial if not promising\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    print(f\"Trial ({trial.number}) | Mean Macro F1 Score ± St. Dev. -- {mean_macro_f1:.4f} ± {np.std(trial_macro_f1_score):.4f}\")\n",
    "    \n",
    "    return mean_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-10 20:39:56,622] Trial 59 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study statistics: \n",
      "  Number of finished trials:  60\n",
      "  Number of pruned trials:  52\n",
      "  Number of complete trials:  8\n",
      "Best trial:\n",
      "  Value:  0.9687532354545388\n",
      "  Params: \n",
      "    colsample_bytree: 0.6749537326452014\n",
      "    boosting_type: gbdt\n",
      "    num_leaves: 143\n",
      "    max_depth: 97\n",
      "    learning_rate: 0.023269571043269426\n",
      "    min_split_gain: 0.4728979135614666\n",
      "    min_child_weight: 3.532669502811908\n",
      "    min_child_samples: 291\n",
      "    reg_alpha: 8.619857698099343e-06\n",
      "    reg_lambda: 0.015262204225165513\n",
      "    feature_fraction: 0.8837732391319297\n",
      "    bagging_fraction: 0.8705361427831009\n",
      "    bagging_freq: 406\n",
      "    max_bin: 229\n",
      "    cat_l2: 0.026592839167437304\n",
      "    cat_smooth: 1.6020190174477881e-06\n",
      "    feature_fraction_bynode: 0.319478870787565\n",
      "{'colsample_bytree': 0.6749537326452014, 'boosting_type': 'gbdt', 'num_leaves': 143, 'max_depth': 97, 'learning_rate': 0.023269571043269426, 'min_split_gain': 0.4728979135614666, 'min_child_weight': 3.532669502811908, 'min_child_samples': 291, 'reg_alpha': 8.619857698099343e-06, 'reg_lambda': 0.015262204225165513, 'feature_fraction': 0.8837732391319297, 'bagging_fraction': 0.8705361427831009, 'bagging_freq': 406, 'max_bin': 229, 'cat_l2': 0.026592839167437304, 'cat_smooth': 1.6020190174477881e-06, 'feature_fraction_bynode': 0.319478870787565}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    STORAGE = \"sqlite:///db.sqlite3\"\n",
    "    # Turn off optuna log notes.\n",
    "    # optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "    SAMPLER = optuna.samplers.TPESampler(seed=SEED, multivariate=True)\n",
    "    STUDY_NAME = \"test-lightgbm-gpu-cv\"\n",
    "    study = optuna.create_study(storage=STORAGE, \n",
    "                                sampler=SAMPLER,\n",
    "                                study_name=STUDY_NAME, \n",
    "                                direction=\"maximize\",\n",
    "                                pruner=optuna.pruners.SuccessiveHalvingPruner(),\n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "    \n",
    "    # Optimizing for Macro F1-score\n",
    "    # study.optimize(objective, n_trials=10_000, timeout=600, n_jobs=4)\n",
    "    optuna.storages.fail_stale_trials(study)\n",
    "    study.optimize(objective, n_trials=20, n_jobs=1, gc_after_trial=True)\n",
    "    # study.optimize(objective, n_trials=16, n_jobs=1, gc_after_trial=True)\n",
    "    # need 26 to get 100 trials completed\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "    \n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params = study.best_params\n",
    "    with open(f\"./{FILE_NAME}/{TUNED_OR_NO}/{CV_RESULT_DIR}/best_params.txt\", \"w\") as f:\n",
    "        f.write(str(best_params))\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Param importance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'min_child_weight': 0.2122872904385088,\n",
       " 'feature_fraction_bynode': 0.18892830546382378,\n",
       " 'reg_lambda': 0.1263979085176058,\n",
       " 'min_split_gain': 0.09332898115482287,\n",
       " 'max_bin': 0.07875923073382815,\n",
       " 'cat_smooth': 0.07811727312847709,\n",
       " 'max_depth': 0.06317063769427472,\n",
       " 'cat_l2': 0.04337087687607975,\n",
       " 'learning_rate': 0.04271222485531932,\n",
       " 'num_leaves': 0.03190756994939829,\n",
       " 'boosting_type': 0.02488717395467689,\n",
       " 'colsample_bytree': 0.015905265742403467,\n",
       " 'min_child_samples': 0.00016935117062725013,\n",
       " 'reg_alpha': 5.791032015392519e-05}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"     Param importance:\")\n",
    "optuna.importance.get_param_importances(study,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('colsample_bytree', 0.6749537326452014), ('boosting_type', 'gbdt'), ('num_leaves', 143), ('max_depth', 97), ('learning_rate', 0.023269571043269426), ('min_split_gain', 0.4728979135614666), ('min_child_weight', 3.532669502811908), ('min_child_samples', 291), ('reg_alpha', 8.619857698099343e-06), ('reg_lambda', 0.015262204225165513), ('feature_fraction', 0.8837732391319297), ('bagging_fraction', 0.8705361427831009), ('bagging_freq', 406), ('max_bin', 229), ('cat_l2', 0.026592839167437304), ('cat_smooth', 1.6020190174477881e-06), ('feature_fraction_bynode', 0.319478870787565)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.importance.get_param_importances(study,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect() # forces garbage collection\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optunasec_lightgbm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
