{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import gc # garbage collection\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.trial import TrialState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To fix TQDM Warning:\n",
    "# https://stackoverflow.com/questions/75349025/vs-code-jupyter-notebook-iprogress-not-found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gc.collect() # forces garbage collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sklearn.exceptions\n",
    "'''\n",
    "During Optuna's Hyperparameter Optimization, we get this error:\n",
    " UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in\n",
    " labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
    "\n",
    "Even with Stratified KFold, because we calculate precision like:\n",
    "True Positive / Predicted Yes\n",
    "\n",
    "And if we predicted all our samples to be No\n",
    "\n",
    "Then Predicted Yes = 0, resulting in undefined precision -- possibly undefined F-score\n",
    "Link:\n",
    "https://stackoverflow.com/questions/35225369/scikit-learn-error-message-precision-and-f-score-are-ill-defined-and-being-set\n",
    "'''\n",
    "# warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "warnings.filterwarnings(\"ignore\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Seed at file level\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "FILE_NAME = 'XGBoost_Official_No_RFECV_No_PCA'\n",
    "N_FOLDS = 10\n",
    "CV_RESULT_DIR = \"./xgboost_cv_results\"\n",
    "TUNED_OR_NO = \"Tuned_Hyperparameters\" # or Untuned_Hyperparameters\n",
    "N_REPEATS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure directories are created for results\n",
    "os.makedirs(f\"./{FILE_NAME}/{TUNED_OR_NO}/{CV_RESULT_DIR}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "url = './RT_IOT2022_new.csv'\n",
    "data = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '-' with 'None' in categorical columns\n",
    "data['service'] = data['service'].replace('-', 'None')\n",
    "data['proto'] = data['proto'].replace('-', 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the target column\n",
    "target = data['Attack_type']\n",
    "data = data.drop(columns = ['Attack_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size= 0.2, random_state=SEED, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory-saving\n",
    "del data\n",
    "del target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Macro F1 score metric for multiclass classification\n",
    "def macro_f1_score(preds, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    y_pred = np.argmax(preds, axis=1)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return 'macro-f1-score', f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=N_FOLDS, n_repeats=N_REPEATS, random_state=SEED)\n",
    "    \n",
    "    # dtrain = xgb.DMatrix(X_train, label=Y_train)    \n",
    "\n",
    "    param = {\n",
    "        \"verbosity\": 0,\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        # \"eval_metric\": ['merror','mlogloss'],\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        \"device\": \"cuda\",\n",
    "        \"n_jobs\": -1,\n",
    "        \"num_class\": len(np.unique(Y_train)),\n",
    "        # sampling ratio for training data.\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        # sampling according to each tree.\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "        \"seed\" : SEED\n",
    "    }\n",
    "\n",
    "    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 50)\n",
    "        # param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 200)\n",
    "        # minimum child weight, larger the term more conservative the tree.\n",
    "        param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 300)\n",
    "        # param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 300)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "\n",
    "    if param[\"booster\"] == \"dart\":\n",
    "        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "\n",
    "    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, 'test-macro-f1-score')\n",
    "    # Initialize the model\n",
    "    model = XGBClassifier(**param)\n",
    "\n",
    "    # Lists to store metrics\n",
    "    trial_macro_f1_score = []\n",
    "    # trial_bal_accuracy = []\n",
    "\n",
    "\n",
    "    for fold_idx, (train_idx, valid_idx) in enumerate(rskf.split(X_train, Y_train)):\n",
    "        fold_num = fold_idx + 1\n",
    "        \n",
    "        # Split the raw data into training and validation sets for this fold\n",
    "        X_train_fold = X_train.iloc[train_idx].copy()\n",
    "        Y_train_fold = Y_train.iloc[train_idx].copy()\n",
    "        X_valid_fold = X_train.iloc[valid_idx].copy()\n",
    "        Y_valid_fold = Y_train.iloc[valid_idx].copy()\n",
    "        \n",
    "        # Preprocessing within the fold\n",
    "        # Reset index to ensure alignment\n",
    "        X_train_fold.reset_index(drop=True, inplace=True)\n",
    "        X_valid_fold.reset_index(drop=True, inplace=True)\n",
    "        Y_train_fold.reset_index(drop=True, inplace=True)\n",
    "        Y_valid_fold.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        # Standardizing numerical features\n",
    "        num_cols = X_train_fold.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "        \n",
    "        # Initialize the scaler\n",
    "        scaler = StandardScaler()\n",
    "        \n",
    "        # Fit the scaler on the training data\n",
    "        X_train_fold[num_cols] = scaler.fit_transform(X_train_fold[num_cols])\n",
    "        \n",
    "        # Transform the validation data\n",
    "        X_valid_fold[num_cols] = scaler.transform(X_valid_fold[num_cols])\n",
    "        \n",
    "        # Encode the target variable (convert to integer labels)\n",
    "        Y_train_fold = Y_train_fold.astype('category').cat.codes\n",
    "        Y_valid_fold = Y_valid_fold.astype('category').cat.codes\n",
    "\n",
    "        # Identify categorical columns to encode\n",
    "        categorical_cols = ['proto', 'service']\n",
    "        \n",
    "        # Encoding categorical variables using OneHotEncoder\n",
    "        encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "        encoder.fit(X_train_fold[categorical_cols])\n",
    "        \n",
    "        # Transform both training and validation sets\n",
    "        X_train_encoded = encoder.transform(X_train_fold[categorical_cols])\n",
    "        X_valid_encoded = encoder.transform(X_valid_fold[categorical_cols])\n",
    "        \n",
    "        # Convert to DataFrame and reset index\n",
    "        X_train_encoded_df = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out())\n",
    "        X_valid_encoded_df = pd.DataFrame(X_valid_encoded, columns=encoder.get_feature_names_out())\n",
    "        \n",
    "        # Drop original categorical columns\n",
    "        X_train_fold = X_train_fold.drop(categorical_cols, axis=1)\n",
    "        X_valid_fold = X_valid_fold.drop(categorical_cols, axis=1)\n",
    "        \n",
    "        # Concatenate encoded features\n",
    "        X_train_fold = pd.concat([X_train_fold, X_train_encoded_df], axis=1)\n",
    "        X_valid_fold = pd.concat([X_valid_fold, X_valid_encoded_df], axis=1)\n",
    "\n",
    "        del X_train_encoded_df, X_valid_encoded_df\n",
    "        \n",
    "        # Convert data to DMatrix\n",
    "        dtrain = xgb.DMatrix(X_train_fold, label=Y_train_fold)\n",
    "        dvalid = xgb.DMatrix(X_valid_fold, label=Y_valid_fold)\n",
    "        \n",
    "        # Set up evals and evals_result\n",
    "        evals = [(dtrain, 'train'), (dvalid, 'validation')]\n",
    "        evals_result = {}    \n",
    "        del X_train_fold, X_valid_fold\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Train the model\n",
    "        model = xgb.train(\n",
    "            params=param,\n",
    "            dtrain=dtrain,\n",
    "            num_boost_round=1_500,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=300,\n",
    "            feval=macro_f1_score,\n",
    "            evals_result=evals_result,\n",
    "            verbose_eval=True,\n",
    "            # callbacks=[pruning_callback]\n",
    "        )\n",
    "        \n",
    "        # Predict on validation data\n",
    "        y_pred_probs = model.predict(dvalid)\n",
    "        y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "        \n",
    "        # Compute metrics\n",
    "        macro_f1 = f1_score(Y_valid_fold, y_pred, average='macro')\n",
    "        trial_macro_f1_score.append(macro_f1)\n",
    "        \n",
    "        del Y_train_fold, Y_valid_fold\n",
    "        \n",
    "        # Clean up\n",
    "        del dtrain, dvalid, model\n",
    "        gc.collect()\n",
    "        # trial_bal_accuracy.append(bal_acc)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Extract the best score.\n",
    "    # print(\"\\n-------------------------====================-------------------------\\n\")\n",
    "    # print(f\"Trial Number {trial.number} :- xgb_cv_results --> {xgb_cv_results}\")\n",
    "\n",
    "    # print(\"\\n-------------------------====================-------------------------\\n\")\n",
    "    # print(f\"Trial Number {trial.number} :- xgb_cv_results.values --> {xgb_cv_results.values}\")\n",
    "    # print(\"\\n-------------------------====================-------------------------\\n\")\n",
    "    # print(f\"Trial Number {trial.number} :- xgb_cv_results.values[-1] --> {xgb_cv_results.values[-1]}\")\n",
    "    # print(\"\\n-------------------------====================-------------------------\\n\")\n",
    "    # print(f\"Trial Number {trial.number} :- xgb_cv_results['test-macro-f1-score'].values[-1] --> {xgb_cv_results['test-macro-f1-score-mean'].values[-1]}\")\n",
    "    # print(\"\\n-------------------------====================-------------------------\\n\")\n",
    "    # test-macro-f1-score-mean  \n",
    "    # test-macro-f1-score-std  \n",
    "    # Compute average metrics across folds\n",
    "    mean_macro_f1 = np.mean(trial_macro_f1_score)\n",
    "    trial.report(mean_macro_f1, step=N_FOLDS * N_REPEATS)\n",
    "    \n",
    "    # Prune trial if not promising\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    print(f\"Trial ({trial.number}) | Mean Macro F1 Score ± St. Dev. -- {np.mean(trial_macro_f1_score):.4f} ± {np.std(trial_macro_f1_score):.4f}\")\n",
    "    \n",
    "    return mean_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.93521\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96427\n",
      "[1]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.94032\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96575\n",
      "[2]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.93895\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96888\n",
      "[3]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.94261\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96914\n",
      "[4]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.93648\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96912\n",
      "[5]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.94207\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96924\n",
      "[6]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.94189\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96893\n",
      "[7]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.93918\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96807\n",
      "[8]\ttrain-mlogloss:2.56495\ttrain-macro-f1-score:0.93912\tvalidation-mlogloss:2.56495\tvalidation-macro-f1-score:0.96837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-01-02 16:51:32,382] Trial 68 failed with parameters: {'booster': 'gbtree', 'lambda': 1.77065387849577e-08, 'alpha': 0.004967821096291411, 'subsample': 0.6382257492731441, 'colsample_bytree': 0.9394198306102094, 'max_depth': 32, 'min_child_weight': 3, 'eta': 2.0408054231989414e-08, 'gamma': 1.5892376349799093e-07, 'grow_policy': 'depthwise'} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/sd/hj30l0b96zj8lbk9ktvcxvpr0000gn/T/ipykernel_43914/1270282528.py\", line 117, in objective\n",
      "    model = xgb.train(\n",
      "            ^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/xgboost/core.py\", line 726, in inner_f\n",
      "    return func(**kwargs)\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/xgboost/training.py\", line 182, in train\n",
      "    if cb_container.after_iteration(bst, i, dtrain, evals):\n",
      "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/xgboost/callback.py\", line 258, in after_iteration\n",
      "    score: str = model.eval_set(evals, epoch, self.metric, self._output_margin)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/xgboost/core.py\", line 2225, in eval_set\n",
      "    feval_ret = feval(\n",
      "                ^^^^^^\n",
      "  File \"/var/folders/sd/hj30l0b96zj8lbk9ktvcxvpr0000gn/T/ipykernel_43914/595900260.py\", line 5, in macro_f1_score\n",
      "    f1 = f1_score(y_true, y_pred, average='macro')\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
      "    prefer_skip_nested_validation or global_skip_validation\n",
      "       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/metrics/_classification.py\", line 1293, in f1_score\n",
      "    Examples\n",
      "           ^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 186, in wrapper\n",
      "    def wrapper(*args, **kwargs):\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/metrics/_classification.py\", line 1485, in fbeta_score\n",
      "    When ``true positive + false positive + false negative == 0``, f-score\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 186, in wrapper\n",
      "    def wrapper(*args, **kwargs):\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/metrics/_classification.py\", line 1793, in precision_recall_fscore_support\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_param_validation.py\", line 186, in wrapper\n",
      "    def wrapper(*args, **kwargs):\n",
      "                   ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/metrics/_classification.py\", line 539, in multilabel_confusion_matrix\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/multiclass.py\", line 111, in unique_labels\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/multiclass.py\", line 111, in <genexpr>\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/multiclass.py\", line 22, in _unique_multiclass\n",
      "    return cached_unique(xp.asarray(y), xp=xp)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_array_api.py\", line 407, in unique_values\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/numpy/lib/arraysetops.py\", line 274, in unique\n",
      "    ret = _unique1d(ar, return_index, return_inverse, return_counts,\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/numpy/lib/arraysetops.py\", line 336, in _unique1d\n",
      "    ar.sort()\n",
      "KeyboardInterrupt\n",
      "[W 2025-01-02 16:51:32,396] Trial 68 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Optimizing for Macro F1-score\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# study.optimize(objective, n_trials=10_000, timeout=600, n_jobs=4)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optuna\u001b[38;5;241m.\u001b[39mstorages\u001b[38;5;241m.\u001b[39mfail_stale_trials(study)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m pruned_trials \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mget_trials(deepcopy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, states\u001b[38;5;241m=\u001b[39m[TrialState\u001b[38;5;241m.\u001b[39mPRUNED])\n\u001b[1;32m     21\u001b[0m complete_trials \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mget_trials(deepcopy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, states\u001b[38;5;241m=\u001b[39m[TrialState\u001b[38;5;241m.\u001b[39mCOMPLETE])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[16], line 117\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    114\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmacro_f1_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[pruning_callback]\u001b[39;49;00m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;66;03m# Predict on validation data\u001b[39;00m\n\u001b[1;32m    130\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(dvalid)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/xgboost/core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/xgboost/training.py:182\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb_container\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevals\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    185\u001b[0m bst \u001b[38;5;241m=\u001b[39m cb_container\u001b[38;5;241m.\u001b[39mafter_training(bst)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/xgboost/callback.py:258\u001b[0m, in \u001b[0;36mCallbackContainer.after_iteration\u001b[0;34m(self, model, epoch, dtrain, evals)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m name\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset name should not contain `-`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 258\u001b[0m score: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_margin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m metric_score \u001b[38;5;241m=\u001b[39m _parse_eval_str(score)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_history(metric_score, epoch)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/xgboost/core.py:2225\u001b[0m, in \u001b[0;36mBooster.eval_set\u001b[0;34m(self, evals, iteration, feval, output_margin)\u001b[0m\n\u001b[1;32m   2223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feval \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2224\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dmat, evname \u001b[38;5;129;01min\u001b[39;00m evals:\n\u001b[0;32m-> 2225\u001b[0m         feval_ret \u001b[38;5;241m=\u001b[39m \u001b[43mfeval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdmat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2229\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(feval_ret, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2230\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m feval_ret:\n\u001b[1;32m   2231\u001b[0m                 \u001b[38;5;66;03m# pylint: disable=consider-using-f-string\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m, in \u001b[0;36mmacro_f1_score\u001b[0;34m(preds, dtrain)\u001b[0m\n\u001b[1;32m      3\u001b[0m y_true \u001b[38;5;241m=\u001b[39m dtrain\u001b[38;5;241m.\u001b[39mget_label()\n\u001b[1;32m      4\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m f1 \u001b[38;5;241m=\u001b[39m \u001b[43mf1_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmacro\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro-f1-score\u001b[39m\u001b[38;5;124m'\u001b[39m, f1\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m validate_parameter_constraints(\n\u001b[1;32m    207\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[1;32m    208\u001b[0m )\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m--> 213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1293\u001b[0m, in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1145\u001b[0m     {\n\u001b[1;32m   1146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1172\u001b[0m ):\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001b[39;00m\n\u001b[1;32m   1174\u001b[0m \n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;124;03m    recall, where an F1 score reaches its best value at 1 and worst score at 0.\u001b[39;00m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;124;03m    The relative contribution of precision and recall to the F1 score are\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;124;03m    equal. The formula for the F1 score is:\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m \n\u001b[1;32m   1180\u001b[0m \u001b[38;5;124;03m    .. math::\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;124;03m        \\\\text{F1} = \\\\frac{2 * \\\\text{TP}}{2 * \\\\text{TP} + \\\\text{FP} + \\\\text{FN}}\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \n\u001b[1;32m   1183\u001b[0m \u001b[38;5;124;03m    Where :math:`\\\\text{TP}` is the number of true positives, :math:`\\\\text{FN}` is the\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m    number of false negatives, and :math:`\\\\text{FP}` is the number of false positives.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;124;03m    F1 is by default\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;124;03m    calculated as 0.0 when there are no true positives, false negatives, or\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m    false positives.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \n\u001b[1;32m   1189\u001b[0m \u001b[38;5;124;03m    Support beyond :term:`binary` targets is achieved by treating :term:`multiclass`\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03m    and :term:`multilabel` data as a collection of binary problems, one for each\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    label. For the :term:`binary` case, setting `average='binary'` will return\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    F1 score for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;124;03m    and F1 score for both classes are computed, then averaged or both returned (when\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;124;03m    `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;124;03m    F1 score for all `labels` are either returned or averaged depending on the\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;124;03m    `average` parameter. Use `labels` specify the set of labels to calculate F1 score\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;124;03m    for.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m \n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \n\u001b[1;32m   1201\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03m    y_true : 1d array-like, or label indicator array / sparse matrix\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;124;03m        Ground truth (correct) target values.\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \n\u001b[1;32m   1206\u001b[0m \u001b[38;5;124;03m    y_pred : 1d array-like, or label indicator array / sparse matrix\u001b[39;00m\n\u001b[1;32m   1207\u001b[0m \u001b[38;5;124;03m        Estimated targets as returned by a classifier.\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \n\u001b[1;32m   1209\u001b[0m \u001b[38;5;124;03m    labels : array-like, default=None\u001b[39;00m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;124;03m        The set of labels to include when `average != 'binary'`, and their\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;124;03m        order if `average is None`. Labels present in the data can be\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;124;03m        excluded, for example in multiclass classification to exclude a \"negative\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m        class\". Labels not present in the data can be included and will be\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m        \"assigned\" 0 samples. For multilabel targets, labels are column indices.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;124;03m        By default, all labels in `y_true` and `y_pred` are used in sorted order.\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \n\u001b[1;32m   1217\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.17\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m           Parameter `labels` improved for multiclass problem.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    pos_label : int, float, bool or str, default=1\u001b[39;00m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;124;03m        The class to report if `average='binary'` and the data is binary,\u001b[39;00m\n\u001b[1;32m   1222\u001b[0m \u001b[38;5;124;03m        otherwise this parameter is ignored.\u001b[39;00m\n\u001b[1;32m   1223\u001b[0m \u001b[38;5;124;03m        For multiclass or multilabel targets, set `labels=[pos_label]` and\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;124;03m        `average != 'binary'` to report metrics for one label only.\u001b[39;00m\n\u001b[1;32m   1225\u001b[0m \n\u001b[1;32m   1226\u001b[0m \u001b[38;5;124;03m    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \\\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m \u001b[38;5;124;03m            default='binary'\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m \u001b[38;5;124;03m        This parameter is required for multiclass/multilabel targets.\u001b[39;00m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;124;03m        If ``None``, the metrics for each class are returned. Otherwise, this\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;124;03m        determines the type of averaging performed on the data:\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03m        ``'binary'``:\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m            Only report results for the class specified by ``pos_label``.\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;124;03m            This is applicable only if targets (``y_{true,pred}``) are binary.\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;124;03m        ``'micro'``:\u001b[39;00m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m            Calculate metrics globally by counting the total true positives,\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m            false negatives and false positives.\u001b[39;00m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m        ``'macro'``:\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each label, and find their unweighted\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;124;03m            mean.  This does not take label imbalance into account.\u001b[39;00m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;124;03m        ``'weighted'``:\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each label, and find their average weighted\u001b[39;00m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;124;03m            by support (the number of true instances for each label). This\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \u001b[38;5;124;03m            alters 'macro' to account for label imbalance; it can result in an\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m            F-score that is not between precision and recall.\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;124;03m        ``'samples'``:\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each instance, and find their average (only\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;124;03m            meaningful for multilabel classification where this differs from\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m \u001b[38;5;124;03m            :func:`accuracy_score`).\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m \n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m    sample_weight : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \u001b[38;5;124;03m        Sample weights.\u001b[39;00m\n\u001b[1;32m   1253\u001b[0m \n\u001b[1;32m   1254\u001b[0m \u001b[38;5;124;03m    zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m \u001b[38;5;124;03m        Sets the value to return when there is a zero division, i.e. when all\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;124;03m        predictions and labels are negative.\u001b[39;00m\n\u001b[1;32m   1257\u001b[0m \n\u001b[1;32m   1258\u001b[0m \u001b[38;5;124;03m        Notes:\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;124;03m        - If set to \"warn\", this acts like 0, but a warning is also raised.\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m        - If set to `np.nan`, such values will be excluded from the average.\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \n\u001b[1;32m   1262\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.3\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;124;03m           `np.nan` option was added.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m \n\u001b[1;32m   1265\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;124;03m    f1_score : float or array of float, shape = [n_unique_labels]\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m \u001b[38;5;124;03m        F1 score of the positive class in binary classification or weighted\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;124;03m        average of the F1 scores of each class for the multiclass task.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m \n\u001b[1;32m   1271\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;124;03m    fbeta_score : Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;124;03m    precision_recall_fscore_support : Compute the precision, recall, F-score,\u001b[39;00m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;124;03m        and support.\u001b[39;00m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;124;03m    jaccard_score : Compute the Jaccard similarity coefficient score.\u001b[39;00m\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;124;03m    multilabel_confusion_matrix : Compute a confusion matrix for each class or\u001b[39;00m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;124;03m        sample.\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \n\u001b[1;32m   1280\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;124;03m    When ``true positive + false positive + false negative == 0`` (i.e. a class\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;124;03m    is completely absent from both ``y_true`` or ``y_pred``), f-score is\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;124;03m    undefined. In such cases, by default f-score will be set to 0.0, and\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;124;03m    ``UndefinedMetricWarning`` will be raised. This behavior can be modified by\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;124;03m    setting the ``zero_division`` parameter.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \n\u001b[1;32m   1288\u001b[0m \u001b[38;5;124;03m    References\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;124;03m    .. [1] `Wikipedia entry for the F1-score\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m \u001b[38;5;124;03m           <https://en.wikipedia.org/wiki/F1_score>`_.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \n\u001b[0;32m-> 1293\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;124;03m    >>> import numpy as np\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import f1_score\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;124;03m    >>> y_true = [0, 1, 2, 0, 1, 2]\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03m    >>> y_pred = [0, 2, 1, 0, 0, 1]\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124;03m    >>> f1_score(y_true, y_pred, average='macro')\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;124;03m    0.26...\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m    >>> f1_score(y_true, y_pred, average='micro')\u001b[39;00m\n\u001b[1;32m   1302\u001b[0m \u001b[38;5;124;03m    0.33...\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[38;5;124;03m    >>> f1_score(y_true, y_pred, average='weighted')\u001b[39;00m\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;124;03m    0.26...\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124;03m    >>> f1_score(y_true, y_pred, average=None)\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;124;03m    array([0.8, 0. , 0. ])\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03m    >>> # binary classification\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m    >>> y_true_empty = [0, 0, 0, 0, 0, 0]\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m    >>> y_pred_empty = [0, 0, 0, 0, 0, 0]\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;124;03m    >>> f1_score(y_true_empty, y_pred_empty)\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;124;03m    0.0...\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;124;03m    >>> f1_score(y_true_empty, y_pred_empty, zero_division=1.0)\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;124;03m    1.0...\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;124;03m    >>> f1_score(y_true_empty, y_pred_empty, zero_division=np.nan)\u001b[39;00m\n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    nan...\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m \n\u001b[1;32m   1318\u001b[0m \u001b[38;5;124;03m    >>> # multilabel classification\u001b[39;00m\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;124;03m    >>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;124;03m    >>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;124;03m    >>> f1_score(y_true, y_pred, average=None)\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001b[39;00m\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fbeta_score(\n\u001b[1;32m   1325\u001b[0m         y_true,\n\u001b[1;32m   1326\u001b[0m         y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1332\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[1;32m   1333\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(func):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# The dict of parameter constraints is set as an attribute of the function\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# to make it possible to dynamically introspect the constraints for\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# automatic testing.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_skl_parameter_constraints\u001b[39m\u001b[38;5;124m\"\u001b[39m, parameter_constraints)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    187\u001b[0m         global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1485\u001b[0m, in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1337\u001b[0m     {\n\u001b[1;32m   1338\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1366\u001b[0m ):\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the F-beta score.\u001b[39;00m\n\u001b[1;32m   1368\u001b[0m \n\u001b[1;32m   1369\u001b[0m \u001b[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001b[39;00m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;124;03m    reaching its optimal value at 1 and its worst value at 0.\u001b[39;00m\n\u001b[1;32m   1371\u001b[0m \n\u001b[1;32m   1372\u001b[0m \u001b[38;5;124;03m    The `beta` parameter represents the ratio of recall importance to\u001b[39;00m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;124;03m    precision importance. `beta > 1` gives more weight to recall, while\u001b[39;00m\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;124;03m    `beta < 1` favors precision. For example, `beta = 2` makes recall twice\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;124;03m    as important as precision, while `beta = 0.5` does the opposite.\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;124;03m    Asymptotically, `beta -> +inf` considers only recall, and `beta -> 0`\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;124;03m    only precision.\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m \n\u001b[1;32m   1379\u001b[0m \u001b[38;5;124;03m    The formula for F-beta score is:\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m \n\u001b[1;32m   1381\u001b[0m \u001b[38;5;124;03m    .. math::\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m \n\u001b[1;32m   1383\u001b[0m \u001b[38;5;124;03m       F_\\\\beta = \\\\frac{(1 + \\\\beta^2) \\\\text{tp}}\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;124;03m                        {(1 + \\\\beta^2) \\\\text{tp} + \\\\text{fp} + \\\\beta^2 \\\\text{fn}}\u001b[39;00m\n\u001b[1;32m   1385\u001b[0m \n\u001b[1;32m   1386\u001b[0m \u001b[38;5;124;03m    Where :math:`\\\\text{tp}` is the number of true positives, :math:`\\\\text{fp}` is the\u001b[39;00m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;124;03m    number of false positives, and :math:`\\\\text{fn}` is the number of false negatives.\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \n\u001b[1;32m   1389\u001b[0m \u001b[38;5;124;03m    Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\u001b[39;00m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03m    and :term:`multilabel` data as a collection of binary problems, one for each\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;124;03m    label. For the :term:`binary` case, setting `average='binary'` will return\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;124;03m    F-beta score for `pos_label`. If `average` is not `'binary'`, `pos_label` is\u001b[39;00m\n\u001b[1;32m   1393\u001b[0m \u001b[38;5;124;03m    ignored and F-beta score for both classes are computed, then averaged or both\u001b[39;00m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;124;03m    returned (when `average=None`). Similarly, for :term:`multiclass` and\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;124;03m    :term:`multilabel` targets, F-beta score for all `labels` are either returned or\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;124;03m    averaged depending on the `average` parameter. Use `labels` specify the set of\u001b[39;00m\n\u001b[1;32m   1397\u001b[0m \u001b[38;5;124;03m    labels to calculate F-beta score for.\u001b[39;00m\n\u001b[1;32m   1398\u001b[0m \n\u001b[1;32m   1399\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\u001b[39;00m\n\u001b[1;32m   1400\u001b[0m \n\u001b[1;32m   1401\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m \u001b[38;5;124;03m    y_true : 1d array-like, or label indicator array / sparse matrix\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;124;03m        Ground truth (correct) target values.\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m \n\u001b[1;32m   1406\u001b[0m \u001b[38;5;124;03m    y_pred : 1d array-like, or label indicator array / sparse matrix\u001b[39;00m\n\u001b[1;32m   1407\u001b[0m \u001b[38;5;124;03m        Estimated targets as returned by a classifier.\u001b[39;00m\n\u001b[1;32m   1408\u001b[0m \n\u001b[1;32m   1409\u001b[0m \u001b[38;5;124;03m    beta : float\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;124;03m        Determines the weight of recall in the combined score.\u001b[39;00m\n\u001b[1;32m   1411\u001b[0m \n\u001b[1;32m   1412\u001b[0m \u001b[38;5;124;03m    labels : array-like, default=None\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m \u001b[38;5;124;03m        The set of labels to include when `average != 'binary'`, and their\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;124;03m        order if `average is None`. Labels present in the data can be\u001b[39;00m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;124;03m        excluded, for example in multiclass classification to exclude a \"negative\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;124;03m        class\". Labels not present in the data can be included and will be\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;124;03m        \"assigned\" 0 samples. For multilabel targets, labels are column indices.\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;124;03m        By default, all labels in `y_true` and `y_pred` are used in sorted order.\u001b[39;00m\n\u001b[1;32m   1419\u001b[0m \n\u001b[1;32m   1420\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.17\u001b[39;00m\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;124;03m           Parameter `labels` improved for multiclass problem.\u001b[39;00m\n\u001b[1;32m   1422\u001b[0m \n\u001b[1;32m   1423\u001b[0m \u001b[38;5;124;03m    pos_label : int, float, bool or str, default=1\u001b[39;00m\n\u001b[1;32m   1424\u001b[0m \u001b[38;5;124;03m        The class to report if `average='binary'` and the data is binary,\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m \u001b[38;5;124;03m        otherwise this parameter is ignored.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;124;03m        For multiclass or multilabel targets, set `labels=[pos_label]` and\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;124;03m        `average != 'binary'` to report metrics for one label only.\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m \n\u001b[1;32m   1429\u001b[0m \u001b[38;5;124;03m    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \\\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m \u001b[38;5;124;03m            default='binary'\u001b[39;00m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;124;03m        This parameter is required for multiclass/multilabel targets.\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;124;03m        If ``None``, the metrics for each class are returned. Otherwise, this\u001b[39;00m\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;124;03m        determines the type of averaging performed on the data:\u001b[39;00m\n\u001b[1;32m   1434\u001b[0m \n\u001b[1;32m   1435\u001b[0m \u001b[38;5;124;03m        ``'binary'``:\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;124;03m            Only report results for the class specified by ``pos_label``.\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;124;03m            This is applicable only if targets (``y_{true,pred}``) are binary.\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;124;03m        ``'micro'``:\u001b[39;00m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;124;03m            Calculate metrics globally by counting the total true positives,\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;124;03m            false negatives and false positives.\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;124;03m        ``'macro'``:\u001b[39;00m\n\u001b[1;32m   1442\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each label, and find their unweighted\u001b[39;00m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;124;03m            mean.  This does not take label imbalance into account.\u001b[39;00m\n\u001b[1;32m   1444\u001b[0m \u001b[38;5;124;03m        ``'weighted'``:\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each label, and find their average weighted\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124;03m            by support (the number of true instances for each label). This\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;124;03m            alters 'macro' to account for label imbalance; it can result in an\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;124;03m            F-score that is not between precision and recall.\u001b[39;00m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;124;03m        ``'samples'``:\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each instance, and find their average (only\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;124;03m            meaningful for multilabel classification where this differs from\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;124;03m            :func:`accuracy_score`).\u001b[39;00m\n\u001b[1;32m   1453\u001b[0m \n\u001b[1;32m   1454\u001b[0m \u001b[38;5;124;03m    sample_weight : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;124;03m        Sample weights.\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \n\u001b[1;32m   1457\u001b[0m \u001b[38;5;124;03m    zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;124;03m        Sets the value to return when there is a zero division, i.e. when all\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;124;03m        predictions and labels are negative.\u001b[39;00m\n\u001b[1;32m   1460\u001b[0m \n\u001b[1;32m   1461\u001b[0m \u001b[38;5;124;03m        Notes:\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m \n\u001b[1;32m   1463\u001b[0m \u001b[38;5;124;03m        - If set to \"warn\", this acts like 0, but a warning is also raised.\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;124;03m        - If set to `np.nan`, such values will be excluded from the average.\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m \n\u001b[1;32m   1466\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.3\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;124;03m           `np.nan` option was added.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \n\u001b[1;32m   1469\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   1470\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;124;03m    fbeta_score : float (if average is not None) or array of float, shape =\\\u001b[39;00m\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;124;03m        [n_unique_labels]\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;124;03m        F-beta score of the positive class in binary classification or weighted\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;124;03m        average of the F-beta score of each class for the multiclass task.\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m \n\u001b[1;32m   1476\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;124;03m    precision_recall_fscore_support : Compute the precision, recall, F-score,\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03m        and support.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \u001b[38;5;124;03m    multilabel_confusion_matrix : Compute a confusion matrix for each class or\u001b[39;00m\n\u001b[1;32m   1481\u001b[0m \u001b[38;5;124;03m        sample.\u001b[39;00m\n\u001b[1;32m   1482\u001b[0m \n\u001b[1;32m   1483\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m \u001b[38;5;124;03m    When ``true positive + false positive + false negative == 0``, f-score\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;124;03m    returns 0.0 and raises ``UndefinedMetricWarning``. This behavior can be\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m \u001b[38;5;124;03m    modified by setting ``zero_division``.\u001b[39;00m\n\u001b[1;32m   1488\u001b[0m \n\u001b[1;32m   1489\u001b[0m \u001b[38;5;124;03m    References\u001b[39;00m\n\u001b[1;32m   1490\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;124;03m    .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\u001b[39;00m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;124;03m           Modern Information Retrieval. Addison Wesley, pp. 327-328.\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \n\u001b[1;32m   1494\u001b[0m \u001b[38;5;124;03m    .. [2] `Wikipedia entry for the F1-score\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;124;03m           <https://en.wikipedia.org/wiki/F1_score>`_.\u001b[39;00m\n\u001b[1;32m   1496\u001b[0m \n\u001b[1;32m   1497\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;124;03m    >>> import numpy as np\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import fbeta_score\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m    >>> y_true = [0, 1, 2, 0, 1, 2]\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    >>> y_pred = [0, 2, 1, 0, 0, 1]\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;124;03m    >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;124;03m    0.23...\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;124;03m    >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;124;03m    0.33...\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m    >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;124;03m    0.23...\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;124;03m    >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001b[39;00m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;124;03m    >>> y_pred_empty = [0, 0, 0, 0, 0, 0]\u001b[39;00m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;124;03m    >>> fbeta_score(y_true, y_pred_empty,\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m \u001b[38;5;124;03m    ...             average=\"macro\", zero_division=np.nan, beta=0.5)\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m    0.12...\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m     _, _, f, _ \u001b[38;5;241m=\u001b[39m precision_recall_fscore_support(\n\u001b[1;32m   1518\u001b[0m         y_true,\n\u001b[1;32m   1519\u001b[0m         y_pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1526\u001b[0m         zero_division\u001b[38;5;241m=\u001b[39mzero_division,\n\u001b[1;32m   1527\u001b[0m     )\n\u001b[1;32m   1528\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(func):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# The dict of parameter constraints is set as an attribute of the function\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# to make it possible to dynamically introspect the constraints for\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# automatic testing.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_skl_parameter_constraints\u001b[39m\u001b[38;5;124m\"\u001b[39m, parameter_constraints)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    187\u001b[0m         global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1793\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1628\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   1629\u001b[0m     {\n\u001b[1;32m   1630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1660\u001b[0m ):\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \n\u001b[1;32m   1663\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;124;03m    true positives and ``fp`` the number of false positives. The precision is\u001b[39;00m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    intuitively the ability of the classifier not to label a negative sample as\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    positive.\u001b[39;00m\n\u001b[1;32m   1667\u001b[0m \n\u001b[1;32m   1668\u001b[0m \u001b[38;5;124;03m    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;124;03m    true positives and ``fn`` the number of false negatives. The recall is\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;124;03m    intuitively the ability of the classifier to find all the positive samples.\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m \n\u001b[1;32m   1672\u001b[0m \u001b[38;5;124;03m    The F-beta score can be interpreted as a weighted harmonic mean of\u001b[39;00m\n\u001b[1;32m   1673\u001b[0m \u001b[38;5;124;03m    the precision and recall, where an F-beta score reaches its best\u001b[39;00m\n\u001b[1;32m   1674\u001b[0m \u001b[38;5;124;03m    value at 1 and worst score at 0.\u001b[39;00m\n\u001b[1;32m   1675\u001b[0m \n\u001b[1;32m   1676\u001b[0m \u001b[38;5;124;03m    The F-beta score weights recall more than precision by a factor of\u001b[39;00m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;124;03m    ``beta``. ``beta == 1.0`` means recall and precision are equally important.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[38;5;124;03m    The support is the number of occurrences of each class in ``y_true``.\u001b[39;00m\n\u001b[1;32m   1680\u001b[0m \n\u001b[1;32m   1681\u001b[0m \u001b[38;5;124;03m    Support beyond term:`binary` targets is achieved by treating :term:`multiclass`\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;124;03m    and :term:`multilabel` data as a collection of binary problems, one for each\u001b[39;00m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;124;03m    label. For the :term:`binary` case, setting `average='binary'` will return\u001b[39;00m\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;124;03m    metrics for `pos_label`. If `average` is not `'binary'`, `pos_label` is ignored\u001b[39;00m\n\u001b[1;32m   1685\u001b[0m \u001b[38;5;124;03m    and metrics for both classes are computed, then averaged or both returned (when\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;124;03m    `average=None`). Similarly, for :term:`multiclass` and :term:`multilabel` targets,\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;124;03m    metrics for all `labels` are either returned or averaged depending on the `average`\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;124;03m    parameter. Use `labels` specify the set of labels to calculate metrics for.\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \n\u001b[1;32m   1690\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \n\u001b[1;32m   1692\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;124;03m    y_true : 1d array-like, or label indicator array / sparse matrix\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;124;03m        Ground truth (correct) target values.\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \n\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03m    y_pred : 1d array-like, or label indicator array / sparse matrix\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;124;03m        Estimated targets as returned by a classifier.\u001b[39;00m\n\u001b[1;32m   1699\u001b[0m \n\u001b[1;32m   1700\u001b[0m \u001b[38;5;124;03m    beta : float, default=1.0\u001b[39;00m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;124;03m        The strength of recall versus precision in the F-score.\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m \n\u001b[1;32m   1703\u001b[0m \u001b[38;5;124;03m    labels : array-like, default=None\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;124;03m        The set of labels to include when `average != 'binary'`, and their\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;124;03m        order if `average is None`. Labels present in the data can be\u001b[39;00m\n\u001b[1;32m   1706\u001b[0m \u001b[38;5;124;03m        excluded, for example in multiclass classification to exclude a \"negative\u001b[39;00m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;124;03m        class\". Labels not present in the data can be included and will be\u001b[39;00m\n\u001b[1;32m   1708\u001b[0m \u001b[38;5;124;03m        \"assigned\" 0 samples. For multilabel targets, labels are column indices.\u001b[39;00m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;124;03m        By default, all labels in `y_true` and `y_pred` are used in sorted order.\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \n\u001b[1;32m   1711\u001b[0m \u001b[38;5;124;03m        .. versionchanged:: 0.17\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;124;03m           Parameter `labels` improved for multiclass problem.\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \n\u001b[1;32m   1714\u001b[0m \u001b[38;5;124;03m    pos_label : int, float, bool or str, default=1\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;124;03m        The class to report if `average='binary'` and the data is binary,\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;124;03m        otherwise this parameter is ignored.\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;124;03m        For multiclass or multilabel targets, set `labels=[pos_label]` and\u001b[39;00m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m        `average != 'binary'` to report metrics for one label only.\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \n\u001b[1;32m   1720\u001b[0m \u001b[38;5;124;03m    average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None, \\\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;124;03m            default='binary'\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;124;03m        This parameter is required for multiclass/multilabel targets.\u001b[39;00m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;124;03m        If ``None``, the metrics for each class are returned. Otherwise, this\u001b[39;00m\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;124;03m        determines the type of averaging performed on the data:\u001b[39;00m\n\u001b[1;32m   1725\u001b[0m \n\u001b[1;32m   1726\u001b[0m \u001b[38;5;124;03m        ``'binary'``:\u001b[39;00m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;124;03m            Only report results for the class specified by ``pos_label``.\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;124;03m            This is applicable only if targets (``y_{true,pred}``) are binary.\u001b[39;00m\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;124;03m        ``'micro'``:\u001b[39;00m\n\u001b[1;32m   1730\u001b[0m \u001b[38;5;124;03m            Calculate metrics globally by counting the total true positives,\u001b[39;00m\n\u001b[1;32m   1731\u001b[0m \u001b[38;5;124;03m            false negatives and false positives.\u001b[39;00m\n\u001b[1;32m   1732\u001b[0m \u001b[38;5;124;03m        ``'macro'``:\u001b[39;00m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each label, and find their unweighted\u001b[39;00m\n\u001b[1;32m   1734\u001b[0m \u001b[38;5;124;03m            mean.  This does not take label imbalance into account.\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;124;03m        ``'weighted'``:\u001b[39;00m\n\u001b[1;32m   1736\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each label, and find their average weighted\u001b[39;00m\n\u001b[1;32m   1737\u001b[0m \u001b[38;5;124;03m            by support (the number of true instances for each label). This\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;124;03m            alters 'macro' to account for label imbalance; it can result in an\u001b[39;00m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;124;03m            F-score that is not between precision and recall.\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;124;03m        ``'samples'``:\u001b[39;00m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;124;03m            Calculate metrics for each instance, and find their average (only\u001b[39;00m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;124;03m            meaningful for multilabel classification where this differs from\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;124;03m            :func:`accuracy_score`).\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \n\u001b[1;32m   1745\u001b[0m \u001b[38;5;124;03m    warn_for : list, tuple or set, for internal use\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;124;03m        This determines which warnings will be made in the case that this\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;124;03m        function is being used to return only one of its metrics.\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m \u001b[38;5;124;03m    sample_weight : array-like of shape (n_samples,), default=None\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;124;03m        Sample weights.\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m \n\u001b[1;32m   1752\u001b[0m \u001b[38;5;124;03m    zero_division : {\"warn\", 0.0, 1.0, np.nan}, default=\"warn\"\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;124;03m        Sets the value to return when there is a zero division:\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m        - recall: when there are no positive labels\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[38;5;124;03m        - precision: when there are no positive predictions\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;124;03m        - f-score: both\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \n\u001b[1;32m   1759\u001b[0m \u001b[38;5;124;03m        Notes:\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m \n\u001b[1;32m   1761\u001b[0m \u001b[38;5;124;03m        - If set to \"warn\", this acts like 0, but a warning is also raised.\u001b[39;00m\n\u001b[1;32m   1762\u001b[0m \u001b[38;5;124;03m        - If set to `np.nan`, such values will be excluded from the average.\u001b[39;00m\n\u001b[1;32m   1763\u001b[0m \n\u001b[1;32m   1764\u001b[0m \u001b[38;5;124;03m        .. versionadded:: 1.3\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;124;03m           `np.nan` option was added.\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \n\u001b[1;32m   1767\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   1769\u001b[0m \u001b[38;5;124;03m    precision : float (if average is not None) or array of float, shape =\\\u001b[39;00m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;124;03m        [n_unique_labels]\u001b[39;00m\n\u001b[1;32m   1771\u001b[0m \u001b[38;5;124;03m        Precision score.\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \n\u001b[1;32m   1773\u001b[0m \u001b[38;5;124;03m    recall : float (if average is not None) or array of float, shape =\\\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;124;03m        [n_unique_labels]\u001b[39;00m\n\u001b[1;32m   1775\u001b[0m \u001b[38;5;124;03m        Recall score.\u001b[39;00m\n\u001b[1;32m   1776\u001b[0m \n\u001b[1;32m   1777\u001b[0m \u001b[38;5;124;03m    fbeta_score : float (if average is not None) or array of float, shape =\\\u001b[39;00m\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;124;03m        [n_unique_labels]\u001b[39;00m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;124;03m        F-beta score.\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \n\u001b[1;32m   1781\u001b[0m \u001b[38;5;124;03m    support : None (if average is not None) or array of int, shape =\\\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;124;03m        [n_unique_labels]\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;124;03m        The number of occurrences of each label in ``y_true``.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \n\u001b[1;32m   1785\u001b[0m \u001b[38;5;124;03m    Notes\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;124;03m    -----\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;124;03m    When ``true positive + false positive == 0``, precision is undefined.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;124;03m    When ``true positive + false negative == 0``, recall is undefined. When\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;124;03m    ``true positive + false negative + false positive == 0``, f-score is\u001b[39;00m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;124;03m    undefined. In such cases, by default the metric will be set to 0, and\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;124;03m    ``UndefinedMetricWarning`` will be raised. This behavior can be modified\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;124;03m    with ``zero_division``.\u001b[39;00m\n\u001b[0;32m-> 1793\u001b[0m \n\u001b[1;32m   1794\u001b[0m \u001b[38;5;124;03m    References\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m \u001b[38;5;124;03m    .. [1] `Wikipedia entry for the Precision and recall\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;124;03m           <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m \n\u001b[1;32m   1799\u001b[0m \u001b[38;5;124;03m    .. [2] `Wikipedia entry for the F1-score\u001b[39;00m\n\u001b[1;32m   1800\u001b[0m \u001b[38;5;124;03m           <https://en.wikipedia.org/wiki/F1_score>`_.\u001b[39;00m\n\u001b[1;32m   1801\u001b[0m \n\u001b[1;32m   1802\u001b[0m \u001b[38;5;124;03m    .. [3] `Discriminative Methods for Multi-labeled Classification Advances\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;124;03m           in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\u001b[39;00m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;124;03m           Godbole, Sunita Sarawagi\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m \u001b[38;5;124;03m           <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m \n\u001b[1;32m   1807\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m \u001b[38;5;124;03m    >>> import numpy as np\u001b[39;00m\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;124;03m    >>> from sklearn.metrics import precision_recall_fscore_support\u001b[39;00m\n\u001b[1;32m   1811\u001b[0m \u001b[38;5;124;03m    >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m \u001b[38;5;124;03m    >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\u001b[39;00m\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;124;03m    >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\u001b[39;00m\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;124;03m    (0.22..., 0.33..., 0.26..., None)\u001b[39;00m\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;124;03m    >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\u001b[39;00m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;124;03m    (0.33..., 0.33..., 0.33..., None)\u001b[39;00m\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;124;03m    >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\u001b[39;00m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;124;03m    (0.22..., 0.33..., 0.26..., None)\u001b[39;00m\n\u001b[1;32m   1819\u001b[0m \n\u001b[1;32m   1820\u001b[0m \u001b[38;5;124;03m    It is possible to compute per-label precisions, recalls, F1-scores and\u001b[39;00m\n\u001b[1;32m   1821\u001b[0m \u001b[38;5;124;03m    supports instead of averaging:\u001b[39;00m\n\u001b[1;32m   1822\u001b[0m \n\u001b[1;32m   1823\u001b[0m \u001b[38;5;124;03m    >>> precision_recall_fscore_support(y_true, y_pred, average=None,\u001b[39;00m\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;124;03m    ... labels=['pig', 'dog', 'cat'])\u001b[39;00m\n\u001b[1;32m   1825\u001b[0m \u001b[38;5;124;03m    (array([0.        , 0.        , 0.66...]),\u001b[39;00m\n\u001b[1;32m   1826\u001b[0m \u001b[38;5;124;03m     array([0., 0., 1.]), array([0. , 0. , 0.8]),\u001b[39;00m\n\u001b[1;32m   1827\u001b[0m \u001b[38;5;124;03m     array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1829\u001b[0m     _check_zero_division(zero_division)\n\u001b[1;32m   1830\u001b[0m     labels \u001b[38;5;241m=\u001b[39m _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(func):\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# The dict of parameter constraints is set as an attribute of the function\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# to make it possible to dynamically introspect the constraints for\u001b[39;00m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;66;03m# automatic testing.\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_skl_parameter_constraints\u001b[39m\u001b[38;5;124m\"\u001b[39m, parameter_constraints)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    187\u001b[0m         global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/metrics/_classification.py:539\u001b[0m, in \u001b[0;36mmultilabel_confusion_matrix\u001b[0;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/multiclass.py:111\u001b[0m, in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/multiclass.py:111\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/multiclass.py:22\u001b[0m, in \u001b[0;36m_unique_multiclass\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m     20\u001b[0m xp, is_array_api_compliant \u001b[38;5;241m=\u001b[39m get_namespace(y, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m is_array_api_compliant:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cached_unique(xp\u001b[38;5;241m.\u001b[39masarray(y), xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mset\u001b[39m(y)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/sklearn/utils/_array_api.py:407\u001b[0m, in \u001b[0;36munique_values\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/SDR/lib/python3.11/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     ar\u001b[38;5;241m.\u001b[39msort()\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    STORAGE = \"sqlite:///db.sqlite3\"\n",
    "    # Turn off optuna log notes.\n",
    "    # optuna.logging.set_verbosity(optuna.logging.WARN)\n",
    "    SAMPLER = optuna.samplers.TPESampler(seed=SEED, multivariate=True)\n",
    "    STUDY_NAME = \"test-xgboost-gpu-cv\"\n",
    "    study = optuna.create_study(storage=STORAGE, \n",
    "                                sampler=SAMPLER,\n",
    "                                study_name=STUDY_NAME, \n",
    "                                direction=\"maximize\",\n",
    "                                pruner=optuna.pruners.SuccessiveHalvingPruner(),\n",
    "                                load_if_exists=True\n",
    "                                )\n",
    "    \n",
    "    # Optimizing for Macro F1-score\n",
    "    # study.optimize(objective, n_trials=10_000, timeout=600, n_jobs=4)\n",
    "    optuna.storages.fail_stale_trials(study)\n",
    "    study.optimize(objective, n_trials=6, n_jobs=1, gc_after_trial=True)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    # results_string = f\"The overall mean\\u00B1SD of Macro F1 Score is {np.mean(all_trial_macro_f1_score):.4f}\\u00B1{np.std(all_trial_macro_f1_score):.4f}\\n\"\n",
    "    # results_string += f\"The overall mean\\u00B1SD of Balanced Accuracy is {np.mean(all_trial_bal_accuracy):.4f}\\u00B1{np.std(all_trial_bal_accuracy):.4f}\"\n",
    "    # results_file_path = f\"./{FILE_NAME}/{TUNED_OR_NO}/{CV_RESULT_DIR}/MeanAndSTD-Results/overall_mean_std.txt\"\n",
    "    # print(results_string)\n",
    "    \n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    \n",
    "    # Save best parameters\n",
    "    best_params = study.best_params\n",
    "    with open(f\"./{FILE_NAME}/{TUNED_OR_NO}/{CV_RESULT_DIR}/best_params.txt\", \"w\") as f:\n",
    "        f.write(str(best_params))\n",
    "    print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Param importance:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'subsample': np.float64(0.256290741667531),\n",
       " 'alpha': np.float64(0.2143174384498936),\n",
       " 'colsample_bytree': np.float64(0.21258692638653648),\n",
       " 'booster': np.float64(0.1933546327718134),\n",
       " 'lambda': np.float64(0.12345026072422545)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"     Param importance:\")\n",
    "optuna.importance.get_param_importances(study,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('booster', 'gbtree'), ('lambda', 3.8055141510295596e-08), ('alpha', 0.0003730968924446954), ('subsample', 0.9700961167521647), ('colsample_bytree', 0.609809526055973), ('max_depth', 34), ('min_child_weight', 2), ('eta', 9.819176016525168e-05), ('gamma', 3.988653496943247e-06), ('grow_policy', 'depthwise')])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.params.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optuna.importance.get_param_importances(study,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect() # forces garbage collection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
